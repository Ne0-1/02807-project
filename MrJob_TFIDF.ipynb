{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2820303c",
   "metadata": {},
   "source": [
    "## MrJobs TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b49ed61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting TFIDF.py\n"
     ]
    }
   ],
   "source": [
    "%%file TFIDF.py\n",
    "\n",
    "# With inspiration from: \n",
    "# https://github.com/ochaton/mrjob-tfidf/blob/master/run.sh\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.compat import jobconf_from_env\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from math import log\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import re\n",
    "\n",
    "# Splits at words\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "data = pd.read_csv('data/tokens.csv', usecols=[1], names=['i', 'tokens'])\n",
    "unique_tokens = np.unique(np.concatenate([str(line).split() for line in data['tokens']]))\n",
    "# Word 2 index dictionary\n",
    "word2idxDict = dict(zip(unique_tokens, np.arange(len(unique_tokens))))\n",
    "\n",
    "NUMBER_OF_DOCUMENTS = 5572\n",
    "NUMBER_OF_UNIQUE_TOKENS = len(unique_tokens)\n",
    "TFIDF = np.zeros((NUMBER_OF_DOCUMENTS, NUMBER_OF_UNIQUE_TOKENS))\n",
    "\n",
    "class MRTFIDF(MRJob):\n",
    "    \n",
    "    # Mapper1: \n",
    "    # Assigns value 1 to word-document pairs. \n",
    "    def get_words_from_line(self, _, line):\n",
    "        \"\"\"\n",
    "        Maps to: \n",
    "        \n",
    "        Key: tuple: (word, document)\n",
    "        Value: 1\n",
    "        \"\"\"\n",
    "        # splitting docname from text\n",
    "        line = line.split(',')\n",
    "        docname, line = line[0], ' '.join(line[1:])\n",
    "        \n",
    "        # Loop through words in line\n",
    "        for term in WORD_RE.findall(line):\n",
    "            # Key-Value pair\n",
    "            yield (term, docname), 1\n",
    "\n",
    "    # Reducer1: \n",
    "    # Reduces values of identical keys (word-doc pais) by summing\n",
    "    def term_frequency_per_doc(self, term_doc, occurences):\n",
    "        \"\"\"\n",
    "        Reduces: values to sum of values\n",
    "        \"\"\"\n",
    "        term, docname = term_doc[0], term_doc[1]\n",
    "        # summing occurences of terms in each term-doc pair\n",
    "        yield (term, docname), sum(occurences)\n",
    "\n",
    "    # Mapper2: \n",
    "    # Maps all keys of documents to a list of terms and their frequencies\n",
    "    def get_docs_tf(self, term_doc, freq):\n",
    "        \"\"\"\n",
    "        Maps to: \n",
    "        \n",
    "        Key: document\n",
    "        Value: tuple: (term, frequency)\n",
    "        \"\"\"\n",
    "        term, doc = term_doc[0], term_doc[1]\n",
    "        yield doc, (term, freq)\n",
    "\n",
    "    # Reducer2: \n",
    "    # Word-document pairs as keys, \n",
    "    # assigns values (frequency of word in doc, total word_count in doc)\n",
    "    def number_of_terms_per_each_doc(self, doc, term_freqs):\n",
    "        \"\"\"\n",
    "        Key: tuple: (term, doc)\n",
    "        Value: tuple: (frequncy of term, total document wordcount)\n",
    "        \"\"\"        \n",
    "        terms = []\n",
    "        freqs = []\n",
    "        terms_in_doc = 0\n",
    "        for term_freq in term_freqs:\n",
    "            term, freq = term_freq[0], term_freq[1]\n",
    "            terms.append(term)\n",
    "            freqs.append(freq)\n",
    "            terms_in_doc += freq\n",
    "\n",
    "        for i in range(len(terms)):\n",
    "            yield (terms[i], doc), (freqs[i], terms_in_doc)\n",
    "\n",
    "    # Mapper3: \n",
    "    # Maps all keys of words to values of tuple(doc, frequency, total number of words)\n",
    "    def get_terms_per_corpus(self, term_doc, freq_docWords):\n",
    "        \"\"\"\n",
    "        Maps to: \n",
    "        \n",
    "        Key: term\n",
    "        Value: tuple: (doc, frequency, total number of words in doc)\n",
    "        \"\"\"        \n",
    "        term, doc = term_doc[0], term_doc[1]\n",
    "        freq, terms_in_doc = freq_docWords[0], freq_docWords[1]\n",
    "        yield term, (doc, freq, terms_in_doc)\n",
    "\n",
    "    # Reducer3: \n",
    "    # Get all term-doc keys, use values (word_frequency, total_words in doc, number of docs containing term)\n",
    "    def term_appearence_in_corpus(self, term, doc_freq_nwords):\n",
    "        \"\"\"\n",
    "        Key: tuple: (term, doc)\n",
    "        Value: tuple: (frequncy of term, total number of words in doc, total number of docs containing term)\n",
    "        \"\"\"\n",
    "        docs_containing_term = 0\n",
    "        docs = []\n",
    "        freqs = []\n",
    "        terms_in_docs = []\n",
    "        \n",
    "        # Creating lists term-doc pair\n",
    "        for dfn in doc_freq_nwords:\n",
    "            docs_containing_term += 1\n",
    "            docs.append(dfn[0])\n",
    "            freqs.append(dfn[1])\n",
    "            terms_in_docs.append(dfn[2])\n",
    "\n",
    "        for i in range(len(docs)):\n",
    "            yield (term, docs[i]), (freqs[i], terms_in_docs[i], docs_containing_term)\n",
    "\n",
    "    # Mapper4\n",
    "    # Maps the calculated tfidf score based on \n",
    "    # frequncy of term, total number of words in doc, total number of docs containing term\n",
    "    def calculate_tf_idf(self, term_doc, tf_n_df):\n",
    "        \"\"\"\n",
    "        Key: tuple: (term, doc)\n",
    "        Value: TFIDF-value\n",
    "        \"\"\"\n",
    "        term, doc = term_doc[0], term_doc[1]\n",
    "        freqs, terms_in_doc, docs_containing_term = tf_n_df[0], tf_n_df[1], tf_n_df[2]\n",
    "        \n",
    "        # Calculating TF and IDF\n",
    "        TF = (freqs / terms_in_doc)\n",
    "        IDF = log(NUMBER_OF_DOCUMENTS / docs_containing_term)\n",
    "        \n",
    "        # Calculating actual TFIDF. \n",
    "        tfidf = TF * IDF\n",
    "        \n",
    "        # Accessing word index\n",
    "        wordIdx = word2idxDict[term]\n",
    "        \n",
    "        # Inputting TFIDF in matrix\n",
    "        TFIDF[int(doc), wordIdx] = tfidf\n",
    "        \n",
    "        # yield (term, doc), tfidf\n",
    "        \n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.get_words_from_line,\n",
    "                reducer=self.term_frequency_per_doc,\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.get_docs_tf,\n",
    "                reducer=self.number_of_terms_per_each_doc,\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.get_terms_per_corpus,\n",
    "                reducer=self.term_appearence_in_corpus,\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.calculate_tf_idf,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRTFIDF.run()\n",
    "    TFIDF.tofile('data/TFIDF.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c15751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /var/folders/mt/m4qdkk9s0ql0tt_xd7yqltwc0000gn/T/TFIDF.philliphoejbjerg.20221108.190823.018784\n",
      "Running step 1 of 4...\n",
      "Running step 2 of 4...\n",
      "Running step 3 of 4...\n",
      "Running step 4 of 4...\n",
      "job output is in /var/folders/mt/m4qdkk9s0ql0tt_xd7yqltwc0000gn/T/TFIDF.philliphoejbjerg.20221108.190823.018784/output\n",
      "Streaming final output from /var/folders/mt/m4qdkk9s0ql0tt_xd7yqltwc0000gn/T/TFIDF.philliphoejbjerg.20221108.190823.018784/output...\n",
      "Removing temp directory /var/folders/mt/m4qdkk9s0ql0tt_xd7yqltwc0000gn/T/TFIDF.philliphoejbjerg.20221108.190823.018784...\n",
      "CPU times: user 83.9 ms, sys: 28.6 ms, total: 113 ms\n",
      "Wall time: 7.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python3 TFIDF.py data/tokens.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "178bad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tokens = pd.read_csv('data/tokens.csv', usecols=[1], names=['i', 'tokens'])\n",
    "unique_tokens = np.unique(np.concatenate([str(line).split() for line in tokens['tokens']]))\n",
    "# saving number of docs and tokens\n",
    "NUMBER_OF_DOCUMENTS, NUMBER_OF_UNIQUE_TOKENS = 5572, len(unique_tokens)\n",
    "\n",
    "# loading TFIDF file\n",
    "TFIDF = np.fromfile('data/TFIDF.dat', dtype=float)\n",
    "os.remove('data/TFIDF.dat')\n",
    "\n",
    "# reshaping\n",
    "TFIDF = np.reshape(TFIDF, (NUMBER_OF_DOCUMENTS, NUMBER_OF_UNIQUE_TOKENS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee6fed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = pd.DataFrame(TFIDF)\n",
    "TFIDF.columns = unique_tokens\n",
    "\n",
    "TFIDF.to_csv('data/TFIDF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d4f27",
   "metadata": {},
   "source": [
    "### Sparse TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d9d77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aah</th>\n",
       "      <th>aaniy</th>\n",
       "      <th>aaooooright</th>\n",
       "      <th>aathi</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abeg</th>\n",
       "      <th>abel</th>\n",
       "      <th>...</th>\n",
       "      <th>zebra</th>\n",
       "      <th>zed</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtoriu</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows Ã— 5833 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aah  aaniy  aaooooright  aathi   ab  abbey  abdomen  abeg  abel  \\\n",
       "0     0.0  0.0    0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "1     0.0  0.0    0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "2     0.0  0.0    0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "3     0.0  0.0    0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "4     0.0  0.0    0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "...   ...  ...    ...          ...    ...  ...    ...      ...   ...   ...   \n",
       "5567  0.0  0.0    0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "5568  0.0  0.0    0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "5569  0.0  0.0    0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "5570  0.0  0.0    0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "5571  0.0  0.0    0.0          0.0    0.0  0.0    0.0      0.0   0.0   0.0   \n",
       "\n",
       "      ...  zebra  zed  zero  zhong  zindgi  zoe  zogtoriu  zoom  zouk  zyada  \n",
       "0     ...    0.0  0.0   0.0    0.0     0.0  0.0       0.0   0.0   0.0    0.0  \n",
       "1     ...    0.0  0.0   0.0    0.0     0.0  0.0       0.0   0.0   0.0    0.0  \n",
       "2     ...    0.0  0.0   0.0    0.0     0.0  0.0       0.0   0.0   0.0    0.0  \n",
       "3     ...    0.0  0.0   0.0    0.0     0.0  0.0       0.0   0.0   0.0    0.0  \n",
       "4     ...    0.0  0.0   0.0    0.0     0.0  0.0       0.0   0.0   0.0    0.0  \n",
       "...   ...    ...  ...   ...    ...     ...  ...       ...   ...   ...    ...  \n",
       "5567  ...    0.0  0.0   0.0    0.0     0.0  0.0       0.0   0.0   0.0    0.0  \n",
       "5568  ...    0.0  0.0   0.0    0.0     0.0  0.0       0.0   0.0   0.0    0.0  \n",
       "5569  ...    0.0  0.0   0.0    0.0     0.0  0.0       0.0   0.0   0.0    0.0  \n",
       "5570  ...    0.0  0.0   0.0    0.0     0.0  0.0       0.0   0.0   0.0    0.0  \n",
       "5571  ...    0.0  0.0   0.0    0.0     0.0  0.0       0.0   0.0   0.0    0.0  \n",
       "\n",
       "[5572 rows x 5833 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c9893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51848f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
