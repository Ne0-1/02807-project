{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717bf5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import mmh3\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f991438",
   "metadata": {},
   "source": [
    "We start by defining the necessary functions to get document similarity (week 5 of course exercises). Please note that the listhash, minhash, and signatures are gone. We will not be using them in this model as we saw the performance of the model decrease from a 0.94 avg F1 score to 0.76. We believe this is due to the information loss as we minhash. And as we won't minhash there is no need to create hashes at all and therefore, there won't be any signatures. Just q-shingles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b10c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create shingle function\n",
    "def shingles(string:str,q:int):\n",
    "    output = set()\n",
    "    for i in range(len(string)+1):\n",
    "        if i < q:\n",
    "            pass\n",
    "        else:\n",
    "            output.add(' '.join(string[i-q:i]))\n",
    "    return output\n",
    "\n",
    "#create jaccard sim function\n",
    "def jaccard(doc1, doc2):\n",
    "    doc1=set(doc1)\n",
    "    doc2=set(doc2)\n",
    "    intersect = doc1.intersection(doc2)\n",
    "    union = doc1.union(doc2)\n",
    "    if len(union) != 0:\n",
    "        return len(intersect) / len(union)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#create document sim function\n",
    "def similarity(docs:dict):\n",
    "    output = np.zeros((len(docs.keys()),len(docs.keys())))\n",
    "    for key1, value1 in docs.items():\n",
    "        for key2, value2 in docs.items():\n",
    "            if key1 <= key2:\n",
    "                pass\n",
    "            else:\n",
    "                jac_value = jaccard(value1,value2)\n",
    "                output[key1,key2] = jac_value\n",
    "    \n",
    "    return np.tril(output) + np.triu(output.T, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e3126",
   "metadata": {},
   "source": [
    "Now we create the \"training\" loop. The training actually happens as we create the document similarity matrix. This function just evaluates one fold of a cross validation ind gives us the predicted labels for the test set in that fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b22fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_knn(x, y_train,test_idx,low,high,k_neighbours=5):\n",
    "    y_test = []\n",
    "    mask = np.ones(len(x),bool)\n",
    "    mask[y_test] = False\n",
    "    for i in test_idx:\n",
    "        ind = []\n",
    "        temp = np.argpartition(x[i], -k_neighbours)[-k_neighbours:]\n",
    "        temp = np.flip(temp)\n",
    "        for idx in temp:\n",
    "            if idx>=high or idx < low:\n",
    "                ind.append(idx)\n",
    "        topk = x[i][ind]\n",
    "        labels = {j: y_train[j] for j in ind}\n",
    "        ham = 0\n",
    "        spam = 0\n",
    "        for key, value in labels.items():\n",
    "            if value == 'ham':\n",
    "                ham += x[i][key]\n",
    "            if value == 'spam':\n",
    "                spam += x[i][key]\n",
    "        if ham>spam:\n",
    "            y_test.append('ham')\n",
    "        if ham == spam:\n",
    "            y_test.append('ham')\n",
    "        if ham<spam:\n",
    "            y_test.append('spam')\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a59d75",
   "metadata": {},
   "source": [
    "Now we get to a piece of code that we are not really proud of. However, we decided to use no sklearn models in this section (except for evaluating f1-scores and accuracies. This is a two-level 5-fold cross validation. The outer level splits the data into a train and test set. The inner level evaluates - through 5-fold cross validation on the outer training set - the best hyperparameter $q$ and returns that. Then the outer level takes that best $q$ and evaluates the scoring metrics on the outer test set.\n",
    "\n",
    "We also keep the predictions of the outer test set in order to do McNemar tests between the shingle and non-shingle KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f4a19aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [12:12, 146.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "\n",
    "emails = pd.read_csv('data/clean_spam.csv', encoding='latin')\n",
    "num_rows = len(emails)\n",
    "kfold_outer = 5\n",
    "test_set_percent_outer = ((num_rows/kfold_outer)/num_rows)\n",
    "test_size_outer = round(test_set_percent_outer*num_rows)+1\n",
    "\n",
    "#define variables to collect metrics:\n",
    "f1_scores = []\n",
    "accuracies = []\n",
    "predicted = []\n",
    "best_q = 0\n",
    "\n",
    "qs = np.arange(2,9)\n",
    "### Do outer cross validation\n",
    "for cross_val,emails in tqdm(enumerate(pd.read_csv('data/clean_spam.csv',sep=',', chunksize=test_size_outer, encoding='latin'))):\n",
    "    train1 = pd.read_csv('data/clean_spam.csv',sep=',', encoding='latin')\n",
    "    train = train1.drop(emails.index, axis=0)\n",
    "    \n",
    "    train.reset_index(inplace=True)\n",
    "    train = train.drop('index',axis=1)\n",
    "    emails_out = emails.reset_index()\n",
    "    \n",
    "    f1s = {}\n",
    "    accs = {}\n",
    "    \n",
    "    ### Do hyper parameter loop\n",
    "    for q in qs:\n",
    "        #get shingles\n",
    "        email_shingles = {train.index[i]: shingles(train.iloc[:,1][i], q=q) for i in train.index}\n",
    "        #get similarity\n",
    "        email_similarity = similarity(email_shingles)\n",
    "        #do training loop\n",
    "        kfold_inner=5\n",
    "        test_set_percent = ((len(train)/kfold_inner)/len(train))\n",
    "        test_size = round(test_set_percent*len(train))\n",
    "        former_test_idx = 0\n",
    "        f1_inner_mean = []\n",
    "        acc_inner_mean = []\n",
    "        \n",
    "        ### Do inner cross validation\n",
    "        # Take mean of all metric scores\n",
    "        for i in range(kfold_inner):\n",
    "            y_test = train.iloc[:,0][former_test_idx:(i+1)*test_size]\n",
    "            y_idx = y_test.index\n",
    "            mask = np.ones(len(train), bool)\n",
    "            mask[y_idx] = False\n",
    "            y_train = train.iloc[:,0][mask]\n",
    "            y_pred = weighted_knn(email_similarity,y_train,y_idx,former_test_idx,(i+1)*test_size,5)\n",
    "            f1 = f1_score(y_test,y_pred, pos_label='spam')\n",
    "            f1_inner_mean.append(f1)\n",
    "            y_pred1 = y_pred1 + list(y_pred)\n",
    "            acc_inner_mean.append(accuracy_score(y_test, y_pred))\n",
    "            former_test_idx += test_size\n",
    "        f1s[q] = np.mean(f1_inner_mean)\n",
    "        accs[q] = np.mean(acc_inner_mean)\n",
    "        \n",
    "    max_q = max(f1s, key=f1s.get)\n",
    "    emails_out = pd.concat([pd.DataFrame(train),pd.DataFrame(emails_out)])\n",
    "    emails_out = emails_out.drop('index',axis=1)\n",
    "    emails_out = emails_out.reset_index()\n",
    "    emails_out = emails_out.drop('index',axis=1)\n",
    "    email_out_shingles = {emails_out.index[i]: shingles(emails_out.iloc[:,1][i], q=max_q) for i in emails_out.index}\n",
    "    sim_matrix = similarity(email_out_shingles)\n",
    "    \n",
    "    y_test = emails_out.iloc[:,0][-test_size_outer:]\n",
    "    y_idx = y_test.index\n",
    "    mask = np.ones(len(emails_out), bool)\n",
    "    mask[y_idx] = False\n",
    "    y_train = emails_out.iloc[:,0][mask]\n",
    "    y_pred = weighted_knn(sim_matrix,y_train,y_idx,len(emails_out)-test_size_outer,len(emails_out),5)\n",
    "    f1_scores.append(f1_score(y_test,y_pred, pos_label='spam'))\n",
    "    accuracies.append(accuracy_score(y_test,y_pred))\n",
    "    predicted = predicted + list(y_pred)\n",
    "    best_q = max_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "48c5ea0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9096774193548388,\n",
       " 0.916030534351145,\n",
       " 0.8995983935742973,\n",
       " 0.901023890784983,\n",
       " 0.9230769230769231]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858a3db",
   "metadata": {},
   "source": [
    "Now we do the same loop but for KNN without q-shingles. Here we can actually drop the two inner loops as we have no $q$ to optimise for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b3edd846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "emails = pd.read_csv('data/clean_spam.csv', encoding='latin')\n",
    "emails.tokens = emails.tokens.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "aad80640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get bag of words as sets for each email\n",
    "email_bow = {emails.index[i]: set(emails['tokens'][i]) for i in range(len(emails))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9f174623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get similarity between documents\n",
    "email_bow_sim = similarity(email_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7eb97d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9235474006116209, 0.9138576779026217, 0.9011857707509882, 0.9072164948453608, 0.9110320284697508]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "kfold=5\n",
    "f1_scores = []\n",
    "test_set_percent = ((len(emails)/kfold)/len(emails))\n",
    "test_size = round(test_set_percent*len(emails))\n",
    "former_test_idx = 0\n",
    "y_tests = []\n",
    "y_pred2 = []\n",
    "for i in range(kfold):\n",
    "    y_test = emails['label'][former_test_idx:(i+1)*test_size]\n",
    "    y_tests = y_tests + list(y_test)\n",
    "    y_idx = y_test.index\n",
    "    mask = np.ones(len(emails), bool)\n",
    "    mask[y_idx] = False\n",
    "    y_train = emails['label'][mask]\n",
    "    y_pred = weighted_knn(email_bow_sim,y_train,y_idx,former_test_idx,(i+1)*test_size,5)\n",
    "    y_pred2 = y_pred2 + list(y_pred)\n",
    "    f1_scores.append(f1_score(y_test,y_pred, pos_label='spam'))\n",
    "    former_test_idx += test_size\n",
    "print(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d775b22",
   "metadata": {},
   "source": [
    "Now we define the McNemar test function. This function is inspired by Introduction to Machine Learning. Our null-hypothesis is going to be that the two models have the same accuracy.\n",
    "\n",
    "If the p-value is below our level of significanse, $\\alpha=0.05$, we can reject the null-hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2343b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "#define McNemar test (inspired by a function made in Introduction to Machine Learning)\n",
    "def mcnemar(y_true, y_pred1, y_pred2, alpha=0.05):\n",
    "    nn = np.zeros((2,2))\n",
    "    c1 = y_pred1 - y_true == 0\n",
    "    c2 = y_pred2 - y_true == 0\n",
    "\n",
    "    nn[0,0] = sum(c1 & c2)\n",
    "    nn[0,1] = sum(c1 & ~c2)\n",
    "    nn[1,0] = sum(~c1 & c2)\n",
    "    nn[1,1] = sum(~c1 & ~c2)\n",
    "\n",
    "    n = sum(nn.flat);\n",
    "    n12 = nn[0,1]\n",
    "    n21 = nn[1,0]\n",
    "\n",
    "    thetahat = (n12-n21)/n\n",
    "    Etheta = thetahat\n",
    "\n",
    "    Q = n**2 * (n+1) * (Etheta+1) * (1-Etheta) / ( (n*(n12+n21) - (n12-n21)**2) )\n",
    "\n",
    "    p = (Etheta + 1)*0.5 * (Q-1)\n",
    "    q = (1-Etheta)*0.5 * (Q-1)\n",
    "\n",
    "    CI = tuple(lm * 2 - 1 for lm in scipy.stats.beta.interval(1-alpha, a=p, b=q) )\n",
    "\n",
    "    p = 2*scipy.stats.binom.cdf(min([n12,n21]), n=n12+n21, p=0.5)\n",
    "    print(r'Result of McNemars test using $\\alpha$ = ', alpha)\n",
    "    print('Comparison matrix n')\n",
    "    print(nn)\n",
    "    if n12+n21 <= 10:\n",
    "        print('Warning, n12+n21 is low: n12+n21=',(n12+n21))\n",
    "    print(r'$\\theta_hat$: ',thetahat)\n",
    "    print(r'Approximate 1-$\\alpha$ confidence interval of $\\theta$: [$\\theta_L$,$\\theta_U$] = ', CI)\n",
    "    print(r'p-value for two-sided test model 1 and model 2 have same accuracy (exact binomial test): p = ', p)\n",
    "\n",
    "    return thetahat, CI, p "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a6ef1",
   "metadata": {},
   "source": [
    "Because of the design of this function, we need to vstack all y_test and y_pred to accomodate for all folds in the cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "99ce35c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of McNemars test using $\\alpha$ =  0.05\n",
      "Comparison matrix n\n",
      "[[4324.   34.]\n",
      " [1121.   91.]]\n",
      "$\\theta_hat$:  -0.19515260323159783\n",
      "Approximate 1-$\\alpha$ confidence interval of $\\theta$: [$\\theta_L$,$\\theta_U$] =  (-0.20594472551654575, -0.18433712513451816)\n",
      "p-value for two-sided test model 1 and model 2 have same accuracy (exact binomial test): p =  0.0\n"
     ]
    }
   ],
   "source": [
    "np_y_tests = np.zeros(len(y_tests))\n",
    "np_y_pred1 = np.zeros(len(y_tests))\n",
    "np_y_pred2 = np.zeros(len(y_tests))\n",
    "\n",
    "for i in range(len(y_tests)):\n",
    "    if y_tests[i] == 'spam':\n",
    "        np_y_tests[i] = 1\n",
    "    if y_pred1[i] == 'spam':\n",
    "        np_y_pred1[i] = 1\n",
    "    if y_pred2[i] == 'spam':\n",
    "        np_y_pred2[i] = 1\n",
    "\n",
    "\n",
    "mcresults = mcnemar(np_y_tests, np_y_pred1, np_y_pred2, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32678b3a",
   "metadata": {},
   "source": [
    "You can see that the p-value is very much lower than our level of confidence (alpha) of 0.05. Therefore, we can reject the null-hypothesis that the accuracies of the two models are equal!\n",
    "\n",
    "Translated to English, this means that the first model based on signatures is significantly better than the model based on tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f40fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
