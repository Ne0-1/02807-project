{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8946ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "#tqdm_pandas(tqdm())\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d14a9d",
   "metadata": {},
   "source": [
    "## Sources\n",
    "https://www.researchgate.net/figure/Average-daily-number-of-base-stations-visited-by-spammers-red-legitimate-customers_fig1_262330820\n",
    "https://towardsdatascience.com/spam-detection-in-sms-messages-3322e03300f5\n",
    "\n",
    "Could use other features as well:\n",
    "* Number of links\n",
    "* Number of emails\n",
    "* Currency symbols\n",
    "* numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19f0e1",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70cc2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "def preprocess(text):\n",
    "    #Common english stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Substituting urls with \n",
    "    url_regex = r'(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "    text = re.sub(r'http\\S+', '#URL#', str(text)) # Maybe change to single-character-symbol -> shingles\n",
    "    #Tokenize using nltk\n",
    "    word_tokens = word_tokenize(text.lower())\n",
    "    #removing none letter characters and stop words\n",
    "    filtered_sentence = [w for w in word_tokens if w not in stop_words and w.isalpha()]\n",
    "    #Conduct stemming\n",
    "    processed_text = [porter.stem(t) for t in filtered_sentence]\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d98c2",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac780d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6ad4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_mpl():\n",
    "    mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "    return\n",
    "setup_mpl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a798f18",
   "metadata": {},
   "source": [
    "The following analysis will investigate the nature of the spam/ham dataset. More specifically, we will look into how long the typical SMS is for each group respectively. Subsequently, we will investigate the amount of spelling mistakes present in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d5b56b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e102b8bfed9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/clean/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "datasets = ['clean_spam.csv', 'clean_completeSpamAssassin.csv', 'clean_Job_postings.csv', 'clean_NEWS.csv']\n",
    "\n",
    "dfs = {'clean_spam.csv': {'data': [], 'spam': [], 'ham': [], 'type': 'SMS'},\n",
    "       'clean_completeSpamAssassin.csv': {'data': [], 'spam': [], 'ham': [], 'type': 'E-mail'},\n",
    "       'clean_Job_postings.csv': {'data': [], 'fraudulent': [], 'genuine': [], 'type': 'Job postings'},\n",
    "       'clean_NEWS.csv': {'data': [], 'fake': [], 'real': [], 'type': 'News'}}\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    # read data\n",
    "    df = pd.read_csv('../data/clean/' + dataset, encoding= \"ISO-8859-1\")\n",
    "\n",
    "    df.tokens = df.tokens.apply(literal_eval)\n",
    "    df['len'] = df.tokens.str.len()\n",
    "    \n",
    "    # Saving data, and the classes into dictionary\n",
    "    dfs[dataset]['data'] = df\n",
    "    dfs[dataset][list(dfs[dataset].keys())[1]] = df[df.label==list(dfs[dataset].keys())[1]]\n",
    "    dfs[dataset][list(dfs[dataset].keys())[2]] = df[df.label==list(dfs[dataset].keys())[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd832af3",
   "metadata": {},
   "source": [
    "First let us investigate the proportion of ham and spam in the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22934685",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "spam_counts = []\n",
    "ham_counts = []\n",
    "for i in range(2):\n",
    "    total.append(dfs[datasets[i]]['data'].label.value_counts()[0] + dfs[datasets[i]]['data'].label.value_counts()[1])\n",
    "    spam_counts.append(dfs[datasets[i]]['data'].label.value_counts()[0])\n",
    "    ham_counts.append(dfs[datasets[i]]['data'].label.value_counts()[1])    \n",
    "    \n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# figure(figsize=(20, 10), dpi=100)\n",
    "plt.rcParams[\"figure.figsize\"] = [10,5]\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "\n",
    "\n",
    "index = ['SMS', 'Email']\n",
    "df = pd.DataFrame({'spam': spam_counts, 'ham': ham_counts}, index=index)\n",
    "ax = df.plot.bar(rot=0, color=['lightcoral', 'lightblue'])\n",
    "       \n",
    "for i, p in enumerate(ax.patches):\n",
    "    percentage = '{:.1f}%'.format(100 * p.get_height()/total[i%2])\n",
    "    x = p.get_x() + p.get_width() / 4\n",
    "    y = p.get_y() + p.get_height() + 60\n",
    "    ax.annotate(percentage, (x, y), fontsize = 16)  \n",
    "    ax.tick_params(axis='x', which='both', labelsize=20)\n",
    "    ax.tick_params(axis='y', which='both', labelsize=20)  \n",
    "    \n",
    "plt.title('Class distributions', size = 25)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15e827",
   "metadata": {},
   "source": [
    "Getting actual counts of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cd551",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print('Dataset:', dataset)\n",
    "    print(dfs[dataset]['data'].label.value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ba1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total observations SMS\", 4825 + 747) #5572\n",
    "print(\"Total observations Email\", 3952 + 1560) # 5512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f23286",
   "metadata": {},
   "source": [
    "Notably, the dataset illustrated a significant class imbalance, which would probably have to be taken into account when training models for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69757575",
   "metadata": {},
   "source": [
    "The following will analyse the length distribution of spam and ham mails:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f8dde",
   "metadata": {},
   "source": [
    "Checking for log normality in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2102346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "def add_headers(\n",
    "    fig,\n",
    "    *,\n",
    "    row_headers=None,\n",
    "    col_headers=None,\n",
    "    row_pad=1,\n",
    "    col_pad=5,\n",
    "    rotate_row_headers=True,\n",
    "    **text_kwargs\n",
    "):\n",
    "    # Based on https://stackoverflow.com/a/25814386\n",
    "\n",
    "    axes = fig.get_axes()\n",
    "\n",
    "    for ax in axes:\n",
    "        sbs = ax.get_subplotspec()\n",
    "\n",
    "        # Putting headers on cols\n",
    "        if (col_headers is not None) and sbs.is_first_row():\n",
    "            ax.annotate(\n",
    "                col_headers[sbs.colspan.start],\n",
    "                xy=(0.5, 1.05),\n",
    "                xytext=(0, col_pad),\n",
    "                xycoords=\"axes fraction\",\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"baseline\",\n",
    "                **text_kwargs,\n",
    "            )\n",
    "\n",
    "        # Putting headers on rows\n",
    "        if (row_headers is not None) and sbs.is_first_col():\n",
    "            ax.annotate(\n",
    "                row_headers[sbs.rowspan.start],\n",
    "                xy=(-0.1, 0.55),\n",
    "                xytext=(-ax.yaxis.labelpad - row_pad, 0),\n",
    "                xycoords=ax.yaxis.label,\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"right\",\n",
    "                va=\"center\",\n",
    "                rotation=rotate_row_headers * 90,\n",
    "                **text_kwargs,\n",
    "            )\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 2, ncols = 2, figsize=(20, 20), dpi=100)\n",
    "\n",
    "datasets = ['clean_spam.csv', 'clean_completeSpamAssassin.csv']*2\n",
    "label = ['spam', 'spam', 'ham', 'ham']\n",
    "\n",
    "col_headers = ['SMS', 'Email']\n",
    "row_headers = ['Spam', 'Ham']\n",
    "\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    measurements = np.log(list(dfs[datasets[i]][label[i]].len.values))\n",
    "    scipy.stats.probplot(measurements, dist=\"norm\", plot=ax)\n",
    "    ax.set_title(\"Probability plot\", fontsize = 28)\n",
    "    ax.set_xlabel(\"Theoretical quantiles\", fontsize=28)\n",
    "    ax.set_ylabel(\"Ordered Values\", fontsize=28)\n",
    "\n",
    "\n",
    "font_kwargs = dict(fontfamily=\"monospace\", fontweight=\"bold\", size=32)\n",
    "add_headers(fig, col_headers=col_headers, row_headers=row_headers, **font_kwargs)\n",
    "\n",
    "fig.suptitle('Q-Q plot of text length', size = 40, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665655c0",
   "metadata": {},
   "source": [
    "As seen, the observations in the Email dataset follow a log normal distribution, thus they will be plotted in log in the following plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a449efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize=(20, 5), dpi=100)\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "spam_lens = []\n",
    "ham_lens = []\n",
    "\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "\n",
    "    # Accessing correct dataset, and getting word count \n",
    "    \n",
    "    spam_label = list(dfs[datasets[i]].keys())[1]\n",
    "    ham_label = list(dfs[datasets[i]].keys())[2]\n",
    "    \n",
    "    \n",
    "    if i == 1:\n",
    "        spam_len = np.log(list(dfs[datasets[i]][spam_label].len.values))\n",
    "        ham_len = np.log(list(dfs[datasets[i]][ham_label].len.values))\n",
    "    else:\n",
    "        spam_len = list(dfs[datasets[i]][spam_label].len.values)\n",
    "        ham_len = list(dfs[datasets[i]][ham_label].len.values)\n",
    "        \n",
    "    spam_lens.append(spam_len)\n",
    "    ham_lens.append(ham_len)\n",
    "\n",
    "    bins = np.arange(min(spam_len), max(spam_len), 1)\n",
    "    hist, edges = np.histogram(spam_len, bins=bins, density=True)\n",
    "    x = (edges[1:] + edges[:-1])/2\n",
    "    width = bins[1]-bins[0]\n",
    "\n",
    "    ax.bar(x, hist, width=width*0.91, color='lightcoral', label=spam_label, alpha=0.7)\n",
    "    ax.set_ylabel(\"counts\")\n",
    "    ax.set_xlabel(\"k\")\n",
    "    ax.set_title(\"\")\n",
    "    \n",
    "    ax.axvline(np.mean(spam_len), c='red', label='$\\mu_{'+spam_label+'}$', linestyle='dashed')\n",
    "    ax.axvline(np.mean(ham_len), c='darkblue', label='$\\mu_{'+ham_label+'}$', linestyle='dashed')\n",
    "\n",
    "    bins = np.arange(min(ham_len), max(ham_len), 1)\n",
    "    hist, edges = np.histogram(ham_len, bins=bins, density=True)\n",
    "    x = (edges[1:] + edges[:-1])/2\n",
    "    width = bins[1]-bins[0]\n",
    "    ax.bar(x, hist, width=width*0.90, color='lightblue', label=ham_label, alpha=0.7)\n",
    "    ax.tick_params(axis='x', which='both', labelsize=15)\n",
    "    ax.tick_params(axis='y', which='both', labelsize=15)    \n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(\"density\", fontsize = 28)\n",
    "    \n",
    "    if str(dfs[datasets[i]]['type']) == 'E-mail':\n",
    "        ax.set_xlabel(\"word count (Log)\", fontsize = 28)\n",
    "    \n",
    "    else: \n",
    "        ax.set_xlabel(\"word count\", fontsize = 28)\n",
    "    \n",
    "    ax.set_title(str(dfs[datasets[i]]['type']), fontsize = 32) #TODO: maybe change to number of words\n",
    "    \n",
    "  \n",
    "\n",
    "plt.suptitle(\"Word Count Distributions\", fontsize = 40, y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a6f65",
   "metadata": {},
   "source": [
    "By visually inspecting the above figure, it appears that spam SMSes tend to have longer content than ham SMSes.\n",
    "Visually, it is difficult to tell with the other datasets, the following will print the mean and standard deviations useed to create the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2bc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate(datasets[:2]):\n",
    "    \n",
    "    print('Dataset:', dataset)\n",
    "    print()\n",
    "    print(f'Mean of spam: {np.mean(spam_lens[i])}')\n",
    "    print(f'Standard deviation of spam: {np.std(spam_lens[i])}')\n",
    "    print()\n",
    "    print(f'Mean of ham: {np.mean(ham_lens[i])}')\n",
    "    print(f'Standard deviation of ham: {np.std(ham_lens[i])}')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188c0f8",
   "metadata": {},
   "source": [
    "Clearly SMSes are of much smaller word count than the remaining datasets, not surprisingly, as SMSes tend to be less formal than the written text contained in the other datasets.\n",
    "\n",
    "Interestingly, the word count of emails have a much higher standard deviation than the rest.\n",
    "\n",
    "The mean and standard deviations within the datasets, does not seem to depend much on whether the text contains spam or ham."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e650d1",
   "metadata": {},
   "source": [
    "Furthermore, the following will investigate the proportion of spelling mistakes in each group respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbeedc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "spell = SpellChecker()\n",
    "\n",
    "def find_misspelled(row, spell):\n",
    "\n",
    "    text = row.text.lower()\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    misspelled = [word for word in spell.unknown(words) if str(word) not in ['nan']]\n",
    "    return misspelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "for dataset in datasets[:2]:\n",
    "    dfs[dataset]['data']['misspelled'] = dfs[dataset]['data'].progress_apply(lambda row: find_misspelled(row, spell), axis=1)\n",
    "    dfs[dataset]['data']['misspelled_size'] = dfs[dataset]['data']['misspelled'].apply(lambda x: len(x))\n",
    "\n",
    "    # Ratio of misspelled words\n",
    "    dfs[dataset]['data']['misspell_ratio'] = dfs[dataset]['data']['misspelled_size'] / dfs[dataset]['data']['len']\n",
    "    \n",
    "    # Saving dataframe per class again\n",
    "    dfs[dataset][list(dfs[dataset].keys())[1]] = dfs[dataset]['data'][dfs[dataset]['data']['label'] == list(dfs[dataset].keys())[1]]\n",
    "    dfs[dataset][list(dfs[dataset].keys())[2]] = dfs[dataset]['data'][dfs[dataset]['data']['label'] == list(dfs[dataset].keys())[2]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821297f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ce96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio in total data:\n",
    "for dataset in datasets[:2]:\n",
    "    \n",
    "    print(\"Total ratio of spam misspellings in\", dataset+\":\", np.sum(dfs[dataset]['spam']['misspelled_size']) / np.sum(dfs[dataset]['spam']['len']))\n",
    "    print(\"Total ratio of ham misspellings in\", dataset+\":\", np.sum(dfs[dataset]['ham']['misspelled_size']) / np.sum(dfs[dataset]['ham']['len']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4afc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows = 2, ncols = 2, figsize=(20, 20), dpi=100)\n",
    "\n",
    "datasets = ['clean_spam.csv', 'clean_completeSpamAssassin.csv']*2\n",
    "label = ['spam', 'spam', 'ham', 'ham']\n",
    "\n",
    "col_headers = ['SMS', 'Email']\n",
    "row_headers = ['Spam', 'Ham']\n",
    "\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    lst = np.array(dfs[datasets[i]][label[i]].misspelled_size.values)\n",
    "    measurements = np.log(lst[lst != 0])\n",
    "    scipy.stats.probplot(measurements, dist=\"norm\", plot=ax)\n",
    "    ax.set_title(\"Probability plot\", fontsize = 28)\n",
    "    ax.set_xlabel(\"Theoretical quantiles\", fontsize=28)\n",
    "    ax.set_ylabel(\"Ordered Values\", fontsize=28)\n",
    "\n",
    "\n",
    "font_kwargs = dict(fontfamily=\"monospace\", fontweight=\"bold\", size = 32)\n",
    "add_headers(fig, col_headers=col_headers, row_headers=row_headers, **font_kwargs)\n",
    "\n",
    "fig.suptitle('Q-Q plot of misspelling dist.', size = 40, y=0.93)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3900f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize=(20,5), dpi=100)\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "spam_lens = []\n",
    "ham_lens = []\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "\n",
    "    spam_label = list(dfs[datasets[i]].keys())[1]\n",
    "    ham_label = list(dfs[datasets[i]].keys())[2]\n",
    "\n",
    "    if i == 1:\n",
    "        lst = np.array(dfs[datasets[i]][spam_label].misspelled_size.values)\n",
    "        spam_len = np.log(lst[lst != 0])\n",
    "        lst = np.array(dfs[datasets[i]][ham_label].misspelled_size.values)\n",
    "        ham_len = np.log(lst[lst != 0])\n",
    "    else:\n",
    "        spam_len = list(dfs[datasets[i]][spam_label].misspelled_size.values)\n",
    "        ham_len = list(dfs[datasets[i]][ham_label].misspelled_size.values)\n",
    "    \n",
    "    spam_lens.append(spam_len)\n",
    "    ham_lens.append(ham_len)\n",
    "\n",
    "    bins = np.arange(min(spam_len), max(spam_len), 1)\n",
    "    hist, edges = np.histogram(spam_len, bins=bins, density=True)\n",
    "    x = (edges[1:] + edges[:-1])/2\n",
    "    width = bins[1]-bins[0]\n",
    "\n",
    "    # fig, ax = plt.subplots(1, figsize=(10,5), dpi=100)\n",
    "    ax.bar(x, hist, width=width*0.91, color='lightcoral', label=spam_label, alpha=0.7)\n",
    "    ax.set_ylabel(\"counts\")\n",
    "    ax.set_xlabel(\"k\")\n",
    "    ax.set_title(\"\")\n",
    "\n",
    "    ax.axvline(np.mean(spam_len), c='red', label='$\\mu_{'+spam_label+'}$', linestyle='dashed')\n",
    "    ax.axvline(np.mean(ham_len), c='darkblue', label='$\\mu_{'+ham_label+'}$', linestyle='dashed')\n",
    "\n",
    "    bins = np.arange(min(ham_len), max(ham_len), 1)\n",
    "    hist, edges = np.histogram(ham_len, bins=bins, density=True)\n",
    "    x = (edges[1:] + edges[:-1])/2\n",
    "    width = bins[1]-bins[0]\n",
    "    ax.bar(x, hist, width=width*0.90, color='lightblue', label=ham_label, alpha=0.7)\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(\"density\")\n",
    "    ax.set_xlabel(\"Misspelled words\")\n",
    "    ax.set_title(\"Misspelling Distribution\")\n",
    "    \n",
    "    ax.set_ylabel(\"density\", fontsize = 28)\n",
    "    if str(dfs[datasets[i]]['type']) == 'E-mail':\n",
    "        ax.set_xlabel(\"misspelled word count (Log)\", fontsize = 28)\n",
    "        \n",
    "    else: \n",
    "        ax.set_xlabel(\"misspelled word count\", fontsize = 28)\n",
    "        \n",
    "    ax.set_title(str(dfs[datasets[i]]['type']), fontsize = 32) #TODO: maybe change to number of words\n",
    "\n",
    "plt.suptitle(\"Misspelling Distributions\", fontsize = 40, y=1.05)    \n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ad795",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataset in enumerate(datasets[:2]):\n",
    "    \n",
    "    print('Dataset:', dataset)\n",
    "    print()\n",
    "    print(f'Mean of spam: {np.mean(spam_lens[i])}')\n",
    "    print(f'Standard deviation of spam: {np.std(spam_lens[i])}')\n",
    "    print()\n",
    "    print(f'Mean of ham: {np.mean(ham_lens[i])}')\n",
    "    print(f'Standard deviation of ham: {np.std(ham_lens[i])}')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca339c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_freq_dist = {}\n",
    "ham_freq_dist = {}\n",
    "\n",
    "for ds in datasets[:2]:\n",
    "    spam_label = list(dfs[ds].keys())[1]\n",
    "    ham_label = list(dfs[ds].keys())[2]\n",
    "    spam_freq_dist[ds] = nltk.FreqDist(dfs[ds][spam_label].misspelled.explode().values)\n",
    "    ham_freq_dist[ds] = nltk.FreqDist(dfs[ds][ham_label].misspelled.explode().values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff83eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (key, value) in spam_freq_dist.items():\n",
    "    spam_freq_dist[key] = {k: v for k, v in sorted(value.items(), key=lambda item: item[1], reverse=True)  if not type(k) == float}\n",
    "\n",
    "for (key, value) in ham_freq_dist.items():\n",
    "    ham_freq_dist[key] = {k: v for k, v in sorted(value.items(), key=lambda item: item[1], reverse=True)  if not type(k) == float}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7126ec0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update(\n",
    "    {\n",
    "        'text.usetex': False,\n",
    "        'font.family': 'stixgeneral',\n",
    "        'mathtext.fontset': 'stix',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=50\n",
    "\n",
    "for ds in datasets[:2]:    \n",
    "\n",
    "    fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize=(20,10),dpi=100)\n",
    "    \n",
    "    spam_ham = [ham_freq_dist[ds], spam_freq_dist[ds]]\n",
    "    labels = [list(dfs[datasets[i]].keys())[1], list(dfs[datasets[i]].keys())[2]]\n",
    "    colors = ['lightblue', 'coral']\n",
    "\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "\n",
    "        ax.bar(list(spam_ham[i].keys())[:k], list(spam_ham[i].values())[:k], color=colors[i])\n",
    "        ax.title.set_text(labels[i])\n",
    "        ax.tick_params(axis=\"x\", labelsize=12, rotation=90)\n",
    "        \n",
    "    fig.suptitle(dfs[ds]['type'], fontsize=30)\n",
    "    \n",
    "    plt.savefig('wordfreq_'+dfs[ds]['type']+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c0194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\", width=1600, height=800, colormap=\"Oranges\")\n",
    "\n",
    "for ds in datasets[:2]:\n",
    "    spam_cloud = wordcloud.generate_from_frequencies(frequencies=spam_freq_dist[ds])\n",
    "\n",
    "    plt.figure( figsize=(20,10), facecolor='k')\n",
    "    plt.imshow(spam_cloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.title('Misspelled words (spam)')\n",
    "    plt.savefig('misspell_spam_'+dfs[ds]['type']+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c14d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", width=1600, height=800, colormap=\"Blues\")\n",
    "\n",
    "for ds in datasets[:2]:\n",
    "    ham_cloud = wordcloud.generate_from_frequencies(frequencies=ham_freq_dist[ds])\n",
    "\n",
    "    plt.figure( figsize=(20,10), facecolor='k')\n",
    "    plt.imshow(ham_cloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.title('Misspelled words (ham)')\n",
    "    plt.savefig('misspell_ham_'+dfs[ds]['type']+'.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
