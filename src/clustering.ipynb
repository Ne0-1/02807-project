{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a95e014a",
   "metadata": {},
   "source": [
    "# Unsupervised Clustering\n",
    "The following part of the notebook will contain the code used to investigate the potential value of a clustering approach for spam detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ceb7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "import nltk\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.spatial.distance\n",
    "from scipy.special import softmax\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import metrics\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4159b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_mpl():\n",
    "    mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "    return None\n",
    "setup_mpl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b37a909",
   "metadata": {},
   "source": [
    "## Implementation of K-means & K-means ++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac2679",
   "metadata": {},
   "source": [
    "The following two code cells contain the K-means implementation and the centroid initialization scheme, which is K-means ++. The specific approach of the initalization is described in the docstring of the plus-plus function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6bfc6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plus_plus(X, k, random_state=42, verbose=True):\n",
    "    '''\n",
    "    ##########################\n",
    "    #k-means++ initialization#\n",
    "    ##########################\n",
    "    \n",
    "    The approach:\n",
    "         * 1st centroid is chosen uniformly at random from the observations.\n",
    "         * Subsequently, the remaining centroids are chosen from the remaining observation with probability\n",
    "           proportionally with the squared distance to the closest existing centroid\n",
    "    \n",
    "    Parameters:\n",
    "        * X : Observations, X \\in R^{observations x features}\n",
    "        * k : Number of centroids\n",
    "        * random_state : seed\n",
    "    \n",
    "    by Christian Djurhuus\n",
    "    '''\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Allocating memory\n",
    "    centroids = np.zeros((k, X.shape[1]))\n",
    "    \n",
    "    # Sampling first centroid uniformly at random from observations\n",
    "    indicies = [i for i in range(X.shape[0])]\n",
    "    first_idx = np.random.choice(indicies, size=1)\n",
    "    indicies.remove(first_idx)\n",
    "    centroids[0,:] = X[first_idx, :]\n",
    "\n",
    "    # Determining remaining number of centroids:\n",
    "    for i in range(1,k):\n",
    "        if verbose:\n",
    "            print(f'Number of centroids defined: {i+1}')\n",
    "        # Compute distance between all observations and existing centroids\n",
    "        pair_dist = (((np.expand_dims(X[indicies,:], 1)-centroids[:i,:]+1e-06)**2).sum(-1))\n",
    "        #pair_dist = cdist(X[indicies,:], centroids[:i,:], metric='euclidean')\n",
    "\n",
    "        # Probabilities:\n",
    "        if pair_dist.ndim==1:\n",
    "            #only one centroid available. Hence, dist to nearest centroid is just pair dist\n",
    "            dist = pair_dist\n",
    "            #probs = (np.exp(dist) / np.sum(np.exp(dist), axis=0))\n",
    "            probs = dist/dist.sum()\n",
    "        else:\n",
    "            dist = pair_dist.min(axis=1)\n",
    "            #probs = (np.exp(dist) / np.sum(np.exp(dist), axis=0))\n",
    "            probs = dist/dist.sum()\n",
    "\n",
    "\n",
    "        #Selecting one of the remaining observations\n",
    "        selected_idx = np.random.choice(indicies,size=1, p=probs)\n",
    "        indicies.remove(selected_idx)\n",
    "        centroids[i, :] = X[selected_idx, :]\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0426e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, k, random_state=42, tot=1e-4, n_init=10, verbose=True):\n",
    "    '''\n",
    "    ##########################\n",
    "    #k-means#\n",
    "    ##########################\n",
    "    \n",
    "    Function run n_init number of k-means clustering and return the result with smallest inertia.\n",
    "    The implementation have utilized numpy's broadcasting functionalities to increase \n",
    "    computational speed w.r.t computing pairwise distances.\n",
    "    \n",
    "    Parameters:\n",
    "        * X : Observations, X \\in R^{observations x features}\n",
    "        * k : Number of centroids\n",
    "        * random_state : seed\n",
    "        * tot : tolerance criteria for loose convergence criteria\n",
    "        * n_init : number of K-means run \n",
    "    \n",
    "    by Christian Djurhuus\n",
    "    '''\n",
    "    #Substracting mean of data\n",
    "    X -= X.mean(axis=0)\n",
    "\n",
    "    best_inertia = 1e6\n",
    "\n",
    "    #run n_init number of the kmeans algorithm and return clusters with minimum inertia\n",
    "    for i in range(1, n_init+1):\n",
    "        if verbose:\n",
    "            print(f'Kmeans run no. {i}')\n",
    "        \n",
    "        #Determining centroids using kmeans++\n",
    "        centroids = plus_plus(X=X, k=k, random_state=i*random_state, verbose=verbose)\n",
    "\n",
    "        #initial placeholder\n",
    "        prev_centroids = np.zeros(centroids.shape)\n",
    "        assignments = np.zeros(X.shape[0])\n",
    "\n",
    "        #Run until convergence\n",
    "        itr = 1\n",
    "        diff = 1e5\n",
    "        while not np.allclose(prev_centroids, centroids):\n",
    "            if verbose:\n",
    "                print(f'Iteration number: {itr} - diff {diff}')\n",
    "            prev_diff = diff\n",
    "            prev_assignments = assignments\n",
    "\n",
    "            #Using broadcasting to compute pairwise distances between observations and centroids\n",
    "            dists=((np.expand_dims(X, 1)-centroids+1e-06)**2).sum(-1)**0.5\n",
    "            assignments = dists.argmin(axis=1)\n",
    "            prev_centroids = centroids.copy()\n",
    "\n",
    "            #Update centroid position\n",
    "            for idx in range(k):\n",
    "                centroids[idx, :] = X[np.where(assignments==idx)].mean(axis=0)\n",
    "\n",
    "            itr += 1\n",
    "\n",
    "\n",
    "            #Distance between previous centroids and current\n",
    "            diff = ((((prev_centroids - centroids + 1e-6)**2).sum(-1))**0.5).sum()\n",
    "\n",
    "            #Early stopping when converged\n",
    "            if np.array_equal(prev_assignments, assignments): #Check for strict convergence\n",
    "                break\n",
    "\n",
    "            if diff < tot: #Check for loose convergence\n",
    "                break\n",
    "\n",
    "        #Computing inertia\n",
    "        #Sum of squared distance between each sample and its assigned center.\n",
    "        inertia = 0\n",
    "        for idx in range(k):\n",
    "\n",
    "            inertia += np.sum((np.expand_dims(X[np.where(assignments==idx)],1) - centroids[idx, :]) ** 2, axis=0).sum() #squared dist\n",
    "\n",
    "        if inertia < best_inertia:\n",
    "            best_inertia = inertia\n",
    "            if verbose:\n",
    "                print(f'Current best inertia: {best_inertia}')\n",
    "            best_assignments = assignments\n",
    "            best_centroids = centroids\n",
    "\n",
    "    return best_assignments, best_centroids, best_inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be2319",
   "metadata": {},
   "source": [
    "The following function defines the performance metrics utilized within this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf6b78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(X, assign, labels):\n",
    "    ARI = metrics.adjusted_rand_score(labels, assign)\n",
    "    NMI = metrics.adjusted_mutual_info_score(labels, assign)\n",
    "    DBI = metrics.davies_bouldin_score(X, assign)\n",
    "    \n",
    "    print(f'ARI: {ARI:.3f}')\n",
    "    print(f'NMI: {NMI:.3f}')\n",
    "    print(f'DBI: {DBI:.3f}')\n",
    "    print('\\n')\n",
    "    return ARI, NMI, DBI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75643277",
   "metadata": {},
   "source": [
    "## SMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f456527",
   "metadata": {},
   "source": [
    "Reading the cleaned sms data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cad8aad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>str_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nSave up to 70% on Life Insurance.\\nWhy Spend...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[save, life, insur, spend, life, quot, save, e...</td>\n",
       "      <td>save life insur spend life quot save ensur fam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[fight, risk, cancer, url, slim, guarante, los...</td>\n",
       "      <td>fight risk cancer url slim guarante lose lb da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[fight, risk, cancer, url, slim, guarante, los...</td>\n",
       "      <td>fight risk cancer url slim guarante lose lb da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##############################################...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[adult, club, offer, free, membership, instant...</td>\n",
       "      <td>adult club offer free membership instant acces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I thought you might like these:\\n1) Slim Down ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[thought, might, like, slim, guarante, lose, l...</td>\n",
       "      <td>thought might like slim guarante lose lb day u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  \\\n",
       "0  \\nSave up to 70% on Life Insurance.\\nWhy Spend...  spam   \n",
       "1  1) Fight The Risk of Cancer!\\nhttp://www.adcli...  spam   \n",
       "2  1) Fight The Risk of Cancer!\\nhttp://www.adcli...  spam   \n",
       "3  ##############################################...  spam   \n",
       "4  I thought you might like these:\\n1) Slim Down ...  spam   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [save, life, insur, spend, life, quot, save, e...   \n",
       "1  [fight, risk, cancer, url, slim, guarante, los...   \n",
       "2  [fight, risk, cancer, url, slim, guarante, los...   \n",
       "3  [adult, club, offer, free, membership, instant...   \n",
       "4  [thought, might, like, slim, guarante, lose, l...   \n",
       "\n",
       "                                          str_tokens  \n",
       "0  save life insur spend life quot save ensur fam...  \n",
       "1  fight risk cancer url slim guarante lose lb da...  \n",
       "2  fight risk cancer url slim guarante lose lb da...  \n",
       "3  adult club offer free membership instant acces...  \n",
       "4  thought might like slim guarante lose lb day u...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/clean/clean_completeSpamAssassin.csv')\n",
    "df['tokens'] = df['tokens'].apply(literal_eval)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db866407",
   "metadata": {},
   "source": [
    "Vectorizing the documents using TFIDF. To be able to use our own tokenized data as input, an identity tokenizer is defined in given to the sklean TFIDF implementations. This prohibit sklearn from conducting any extra preprocessing, such that we only use our own preprocessing procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85f0e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TDIDF\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer,\n",
    "                             lowercase=False\n",
    "                            )\n",
    "\n",
    "vecs = vectorizer.fit_transform(df['tokens'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = vecs.todense()\n",
    "lst1 = dense.tolist()\n",
    "TDM = pd.DataFrame(lst1, columns=feature_names).dropna()\n",
    "X = vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e713ec3",
   "metadata": {},
   "source": [
    "### Clustering in Original Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214d16e",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d7c0f",
   "metadata": {},
   "source": [
    "First we will choose the most suitable $K$ in K-means by using the obtained inertia with varying values of $K$. The iniertia describes the sum of squared distance between each sample and its assigned centroid. Hence, a low inertia will indicate more well-defined clusters w.r.t inner distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1db84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836c226521384ad2a51665e3a34a1242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inertias = np.zeros(8)\n",
    "best_inertia = 1e5\n",
    "for i, k in tqdm(enumerate(np.arange(2,10,1))):\n",
    "    assign, centroids, inertia = kmeans(X, k=k, random_state=2, tot=1e-4, verbose=False)\n",
    "    inertias[i] = inertia\n",
    "    if inertia < best_inertia:\n",
    "        best_assign = assign\n",
    "        best_centroids = centroids\n",
    "        best_inertia = inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e612034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(np.arange(2,10,1), inertias)\n",
    "ax.set_xlabel('K')\n",
    "ax.set_ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dca81c",
   "metadata": {},
   "source": [
    "Usually, we would choose the value of $K$ associated with the highest curvature. However, given that curve resembles a straight line and only marginal gains appear with increasing $K$, we decide to use $K=2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87834fa",
   "metadata": {},
   "source": [
    "Subsequently, we will choose the most suitable $\\epsilon$ value used in DBSCAN by visualising the distance between the n nearest neighbours as a function of n. $\\epsilon$ is choosed where the curve express the highest curvature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "n_neighbors = 10\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "neighbors = nearest_neighbors.fit(X)\n",
    "distances, indices = neighbors.kneighbors(X)\n",
    "distances = np.sort(distances[:,n_neighbors-1], axis=0)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d6559",
   "metadata": {},
   "source": [
    "This approach appeared to be a bit more challenging than selecting $K$, we see that the highest curvature is with a very small epsilon value. Different epsilon values was also tried and the best appeared to be $\\epsilon=0.1$, this is also true for all the other experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728944fb",
   "metadata": {},
   "source": [
    "#### Performing Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49354abc",
   "metadata": {},
   "source": [
    "Conducting the clustering using the hyperparameters identified above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4ded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "kmeans_assign, centroids, _ = kmeans(X, k=2, random_state=2, tot=1e-4, verbose=False)\n",
    "clustering_db = DBSCAN(eps=0.1, min_samples=2).fit(X)\n",
    "dbscan_assign = clustering_db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f351835c",
   "metadata": {},
   "source": [
    "Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.label.replace({'ham':0, 'spam':1}).values\n",
    "performance_metrics(X.toarray(), kmeans_assign, labels)\n",
    "performance_metrics(X.toarray(), dbscan_assign, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e9cd8",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fccaac",
   "metadata": {},
   "source": [
    "Computing the truncated SVD that approximately explains 50% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f11a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=300)\n",
    "X_svd = svd.fit_transform(X)\n",
    "var_explained = svd.explained_variance_ratio_.sum()\n",
    "print(var_explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2998ddf",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0510b6",
   "metadata": {},
   "source": [
    "Selecting $K$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = np.zeros(8)\n",
    "best_inertia = 1e5\n",
    "for i, k in enumerate(np.arange(2,10,1)):\n",
    "    assign, centroids, inertia = kmeans(X_svd, k=k, random_state=2, tot=1e-4, verbose=False)\n",
    "    inertias[i] = inertia\n",
    "    if inertia < best_inertia:\n",
    "        best_assign = assign\n",
    "        best_centroids = centroids\n",
    "        best_inertia = inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(np.arange(2,10,1), inertias)\n",
    "ax.set_xlabel('K')\n",
    "ax.set_ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543fb1f",
   "metadata": {},
   "source": [
    "Selecting $\\epsilon$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0d7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "n_neighbors = 10\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "neighbors = nearest_neighbors.fit(X_svd)\n",
    "distances, indices = neighbors.kneighbors(X_svd)\n",
    "distances = np.sort(distances[:,n_neighbors-1], axis=0)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc39c3",
   "metadata": {},
   "source": [
    "#### Performing Clustering\n",
    "Conducting clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3072b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_db = DBSCAN(eps=0.1, min_samples=2).fit(X_svd)\n",
    "dbscan_assign = clustering_db.labels_\n",
    "kmeans_assign, centroids, _ = kmeans(X_svd, k=2, random_state=2, tot=1e-4, n_init=10, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb737a81",
   "metadata": {},
   "source": [
    "Evaluating performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.label.replace({'ham':0, 'spam':1}).values\n",
    "performance_metrics(X_svd, kmeans_assign, labels)\n",
    "performance_metrics(X_svd, dbscan_assign, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b8559",
   "metadata": {},
   "source": [
    "### Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4a8ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "dist = pairwise_distances(X, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c08e9f",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "Selecting $K$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = np.zeros(8)\n",
    "best_inertia = 1e5\n",
    "for i, k in tqdm(enumerate(np.arange(2,10,1))):\n",
    "    assign, centroids, inertia = kmeans(dist, k=k, random_state=2, tot=1e-4, verbose=False)\n",
    "    inertias[i] = inertia\n",
    "    if inertia < best_inertia:\n",
    "        best_assign = assign\n",
    "        best_centroids = centroids\n",
    "        best_inertia = inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(np.arange(2,10,1), inertias)\n",
    "ax.set_xlabel('K')\n",
    "ax.set_ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4114e59",
   "metadata": {},
   "source": [
    "Selecting $\\epsilon$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=100)\n",
    "neighbors = nearest_neighbors.fit(dist)\n",
    "distances, indices = neighbors.kneighbors(dist)\n",
    "distances = np.sort(distances[:,10], axis=0)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Distance\")\n",
    "#plt.xlim([0, 500])\n",
    "plt.savefig(\"Distance_curve.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519b667",
   "metadata": {},
   "source": [
    "#### Performing Clustering\n",
    "Conducting clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_assign, centroids_kmeans, _ = kmeans(dist, k=2, random_state=2, tot=0.0001, n_init=10, verbose=False)\n",
    "dist = pairwise_distances(X, metric='cosine')\n",
    "clustering_db = DBSCAN(eps=0.1, min_samples=2, metric='precomputed').fit(dist)\n",
    "dbscan_assign = clustering_db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb05351",
   "metadata": {},
   "source": [
    "Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d3dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.label.replace({'ham':0, 'spam':1}).values\n",
    "performance_metrics(dist, kmeans_assign, labels)\n",
    "performance_metrics(dist, dbscan_assign, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf37e5",
   "metadata": {},
   "source": [
    "### Spectral Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7481cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d8d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define affinity matrix\n",
    "A = similarity\n",
    "\n",
    "#Setting diagonal elements to 0.0 and removing documents with no similarity with other documents\n",
    "np.fill_diagonal(A, 0.0)\n",
    "row_sum = A.sum(0)\n",
    "removed_idx = np.where(A.sum(0)==0)[0]\n",
    "A_reduced = A[row_sum != 0, :]\n",
    "A_reduced = A_reduced[:, row_sum!=0]\n",
    "\n",
    "assert np.where(A_reduced.sum(0)==0)[0].size == 0\n",
    "assert np.where(A_reduced.sum(1)==0)[0].size == 0\n",
    "\n",
    "# Define D as a diagonal matrix where element (i,i) corresponds to the sum of ith row in A\n",
    "D = np.diag(A_reduced.sum(axis=0))\n",
    "D_pow = np.diag(np.power(np.diagonal(D), -0.5))\n",
    "# Define L=D^{-1/2}AD^{-1/2}\n",
    "L = D_pow@A_reduced@D_pow\n",
    "\n",
    "# Find the k largest eigenvectors of L\n",
    "eigenvals, eigenvectors = np.linalg.eigh(L)\n",
    "\n",
    "k = 10\n",
    "k_largest = np.argpartition(eigenvals, -k)[-k:]\n",
    "\n",
    "#Create matrix X_spec = [x1,...,xk]\n",
    "X_spec = eigenvectors[:, k_largest]\n",
    "\n",
    "# Define Y as X row normalized\n",
    "Y = X_spec/np.linalg.norm(X_spec, axis=0)\n",
    "\n",
    "row_sums = np.power(X_spec, 2).sum(axis=1)\n",
    "#row_sums = X_spec.sum(axis=1)\n",
    "Y = (X_spec / np.power(row_sums[:, np.newaxis], 0.5))\n",
    "#Y = X_spec / row_sums[:, np.newaxis]\n",
    "Y = np.nan_to_num(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00b738",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "Choosing $K$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6efe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = np.zeros(8)\n",
    "best_inertia = 1e5\n",
    "for i, k in enumerate(np.arange(2,10,1)):\n",
    "    assign, centroids, inertia = kmeans(Y, k=k, random_state=2, tot=1e-4)\n",
    "    inertias[i] = inertia\n",
    "    if inertia < best_inertia:\n",
    "        best_assign = assign\n",
    "        best_centroids = centroids\n",
    "        best_inertia = inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d1230",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(np.arange(2,10,1), inertias, c='darkblue')\n",
    "ax.set_xlabel('K')\n",
    "ax.set_ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04acf86a",
   "metadata": {},
   "source": [
    "#### Conducting clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e845f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_spec_assign, centroids, _ = kmeans(Y, k=2, random_state=2, tot=1e-4, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483e502b",
   "metadata": {},
   "source": [
    "Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d76b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.drop(removed_idx).label.replace({'ham':0, 'spam':1}).values\n",
    "performance_metrics(Y, kmeans_spec_assign, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0205f97",
   "metadata": {},
   "source": [
    "## Emails\n",
    "This section will conduct the exact same procedure as the above. Hence, the number of comments will be limited and for further explainations please see the section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean/clean_completeSpamAssassin.csv')\n",
    "df['tokens'] = df['tokens'].apply(literal_eval)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5d50b",
   "metadata": {},
   "source": [
    "### Clustering in Original Space\n",
    "Choosing $K$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fe48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = np.zeros(8)\n",
    "best_inertia = 1e5\n",
    "for i, k in tqdm(enumerate(np.arange(2,10,1))):\n",
    "    assign, centroids, inertia = kmeans(X, k=k, random_state=2, tot=1e-4, verbose=False)\n",
    "    inertias[i] = inertia\n",
    "    if inertia < best_inertia:\n",
    "        best_assign = assign\n",
    "        best_centroids = centroids\n",
    "        best_inertia = inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cff9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(np.arange(2,10,1), inertias)\n",
    "ax.set_xlabel('K')\n",
    "ax.set_ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52477c0f",
   "metadata": {},
   "source": [
    "Choosing $\\epsilon$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f48b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "n_neighbors = 10\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "neighbors = nearest_neighbors.fit(X)\n",
    "distances, indices = neighbors.kneighbors(X)\n",
    "distances = np.sort(distances[:,n_neighbors-1], axis=0)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64465a9",
   "metadata": {},
   "source": [
    "#### Performing Clustering\n",
    "Conducting clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2508f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_assign, centroids, _ = kmeans(X, k=2, random_state=2, tot=1e-4, verbose=False)\n",
    "clustering_db = DBSCAN(eps=0.1, min_samples=2).fit(X)\n",
    "dbscan_assign = clustering_db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dedcd4a",
   "metadata": {},
   "source": [
    "Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32bb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.label.replace({'ham':0, 'spam':1}).values\n",
    "performance_metrics(X.toarray(), kmeans_assign, labels)\n",
    "performance_metrics(X.toarray(), dbscan_assign, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664609ce",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=)\n",
    "X_svd = svd.fit_transform(X)\n",
    "var_explained = svd.explained_variance_ratio_.sum()\n",
    "print(var_explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c76b18",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "Choosing $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2af7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = np.zeros(8)\n",
    "best_inertia = 1e5\n",
    "for i, k in enumerate(np.arange(2,10,1)):\n",
    "    assign, centroids, inertia = kmeans(X_svd, k=k, random_state=2, tot=1e-4)\n",
    "    inertias[i] = inertia\n",
    "    if inertia < best_inertia:\n",
    "        best_assign = assign\n",
    "        best_centroids = centroids\n",
    "        best_inertia = inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770197a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(np.arange(2,10,1), inertias)\n",
    "ax.set_xlabel('K')\n",
    "ax.set_ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d05fcc",
   "metadata": {},
   "source": [
    "Choosing $\\epsilon$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfea144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=100)\n",
    "neighbors = nearest_neighbors.fit(X_svd)\n",
    "distances, indices = neighbors.kneighbors(X_svd)\n",
    "distances = np.sort(distances[:,10], axis=0)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.savefig(\"Distance_curve.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2a415",
   "metadata": {},
   "source": [
    "### Performing clustering\n",
    "Conducting clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521dd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_assign, centroids, _ = kmeans(X_svd, k=4, random_state=2, tot=1e-4, verbose=False)\n",
    "\n",
    "clustering_db = DBSCAN(eps=0.1, min_samples=2).fit(X_svd)\n",
    "dbscan_assign = clustering_db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ea13f",
   "metadata": {},
   "source": [
    "Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.label.replace({'ham':0, 'spam':1}).values\n",
    "performance_metrics(X_svd, kmeans_assign, labels)\n",
    "performance_metrics(X_svd, dbscan_assign, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc13208b",
   "metadata": {},
   "source": [
    "### Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4aff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pairwise_distances(X, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54798e7b",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "Choosing $K$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = np.zeros(8)\n",
    "best_inertia = 1e5\n",
    "for i, k in enumerate(np.arange(2,10,1)):\n",
    "    assign, centroids, inertia = kmeans(dist, k=k, random_state=2, tot=1e-4)\n",
    "    inertias[i] = inertia\n",
    "    if inertia < best_inertia:\n",
    "        best_assign = assign\n",
    "        best_centroids = centroids\n",
    "        best_inertia = inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(np.arange(2,10,1), inertias)\n",
    "ax.set_xlabel('K')\n",
    "ax.set_ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7e3bb",
   "metadata": {},
   "source": [
    "Choosing $\\epsilon$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49db5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=100)\n",
    "neighbors = nearest_neighbors.fit(dist)\n",
    "distances, indices = neighbors.kneighbors(dist)\n",
    "distances = np.sort(distances[:,10], axis=0)\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.plot(distances)\n",
    "plt.xlabel(\"Points\")\n",
    "plt.ylabel(\"Distance\")\n",
    "#plt.xlim([0, 500])\n",
    "plt.savefig(\"Distance_curve.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a746c",
   "metadata": {},
   "source": [
    "#### Performing Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2bdcc4",
   "metadata": {},
   "source": [
    "Conducting clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce091ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_assign, centroids_kmeans, _ = kmeans(dist, k=4, random_state=2, tot=0.0001, n_init=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54731f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pairwise_distances(X, metric='cosine')\n",
    "clustering_db = DBSCAN(eps=0.1, min_samples=2, metric='precomputed').fit(dist)\n",
    "dbscan_assign = clustering_db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960c837d",
   "metadata": {},
   "source": [
    "Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.label.replace({'ham':0, 'spam':1}).values\n",
    "performance_metrics(dist, kmeans_assign, labels)\n",
    "performance_metrics(dist, dbscan_assign, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f5190b",
   "metadata": {},
   "source": [
    "### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4438f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define affinity matrix\n",
    "A = similarity\n",
    "\n",
    "#Setting diagonal elements to 0.0 and removing documents with no similarity with other documents\n",
    "np.fill_diagonal(A, 0.0)\n",
    "row_sum = A.sum(0)\n",
    "removed_idx = np.where(A.sum(0)==0)[0]\n",
    "A_reduced = A[row_sum != 0, :]\n",
    "A_reduced = A_reduced[:, row_sum!=0]\n",
    "\n",
    "assert np.where(A_reduced.sum(0)==0)[0].size == 0\n",
    "assert np.where(A_reduced.sum(1)==0)[0].size == 0\n",
    "\n",
    "# Define D as a diagonal matrix where element (i,i) corresponds to the sum of ith row in A\n",
    "D = np.diag(A_reduced.sum(axis=0))\n",
    "D_pow = np.diag(np.power(np.diagonal(D), -0.5))\n",
    "# Define L=D^{-1/2}AD^{-1/2}\n",
    "L = D_pow@A_reduced@D_pow\n",
    "\n",
    "# Find the k largest eigenvectors of L\n",
    "eigenvals, eigenvectors = np.linalg.eigh(L)\n",
    "\n",
    "k = 10\n",
    "k_largest = np.argpartition(eigenvals, -k)[-k:]\n",
    "\n",
    "#Create matrix X_spec = [x1,...,xk]\n",
    "X_spec = eigenvectors[:, k_largest]\n",
    "\n",
    "# Define Y as X row normalized\n",
    "Y = X_spec/np.linalg.norm(X_spec, axis=0)\n",
    "\n",
    "row_sums = np.power(X_spec, 2).sum(axis=1)\n",
    "#row_sums = X_spec.sum(axis=1)\n",
    "Y = (X_spec / np.power(row_sums[:, np.newaxis], 0.5))\n",
    "#Y = X_spec / row_sums[:, np.newaxis]\n",
    "Y = np.nan_to_num(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b5fa5",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning\n",
    "Choosing $K$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb492d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = np.zeros(8)\n",
    "best_inertia = 1e5\n",
    "for i, k in enumerate(np.arange(2,10,1)):\n",
    "    assign, centroids, inertia = kmeans(Y, k=k, random_state=2, tot=1e-4)\n",
    "    inertias[i] = inertia\n",
    "    if inertia < best_inertia:\n",
    "        best_assign = assign\n",
    "        best_centroids = centroids\n",
    "        best_inertia = inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76027a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(np.arange(2,10,1), inertias, c='darkblue')\n",
    "ax.set_xlabel('K')\n",
    "ax.set_ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124924c1",
   "metadata": {},
   "source": [
    "#### Performing Clustering\n",
    "Conducting clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_spec_assign, centroids, _ = kmeans(Y, k=3, random_state=2, tot=1e-4, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5bd8d3",
   "metadata": {},
   "source": [
    "Evaluating performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90731c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.drop(removed_idx).label.replace({'ham':0, 'spam':1}).values\n",
    "performance_metrics(Y, kmeans_spec_assign, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490592f",
   "metadata": {},
   "source": [
    "## Visualizing Clustering Results\n",
    "The following section will visualize the best obtained clustering results (Spectral clustering with K-means) wil be visualised and analysed in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea484bc",
   "metadata": {},
   "source": [
    "### Reorganized Affinity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50088cab",
   "metadata": {},
   "source": [
    "The following is the reorganized pairwise cosine similarity matrix both using the ground truth partition and the cluster allocations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e222b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = labels.argsort()\n",
    "\n",
    "_, group_len = np.unique(labels, return_counts=True)\n",
    "\n",
    "\n",
    "D = A_reduced[:, new_order][new_order]\n",
    "\n",
    "fig, (ax1) = plt.subplots(figsize=(10,10), dpi=100)\n",
    "ax1.spy(D, precision=0.5, markersize=0.5)\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "y = labels\n",
    "N = A_reduced.shape[0]\n",
    "counts=np.unique(y,return_counts=True)[1]\n",
    "counts_j=np.unique(y,return_counts=True)[1]\n",
    "cum_i=np.cumsum(counts)\n",
    "cum_j=np.cumsum(counts_j)\n",
    "\n",
    "for i in range(cum_i.shape[0]):\n",
    "    x1=np.array([cum_i[i],cum_i[i]])\n",
    "    y1=np.array([[0,N]])\n",
    "    line = plt.Line2D(y1, x1, lw=1.5, color='k', alpha=0.8)\n",
    "    ax1.add_line(line)\n",
    "\n",
    "for i in range(cum_i.shape[0]):\n",
    "    x1=np.array([cum_j[i],cum_j[i]])\n",
    "    y1=np.array([[0,N]])\n",
    "    line = plt.Line2D(x1, y1, lw=1.5, color='k', alpha=0.8)\n",
    "    ax1.add_line(line)\n",
    "\n",
    "fig.savefig('sim_matrix_ground_truth.png', dpi=100)\n",
    "\n",
    "fig, (ax2) = plt.subplots(figsize=(10,10), dpi=100)\n",
    "\n",
    "freq = np.flip(np.argsort(np.bincount(kmeans_spec_assign))[-(np.unique(kmeans_spec_assign).size):])\n",
    "new_order = np.hstack((np.where(kmeans_spec_assign==freq[0])[0], \n",
    "                       np.where(kmeans_spec_assign==freq[1])[0],\n",
    "                       np.where(kmeans_spec_assign==freq[2])[0]))\n",
    "\n",
    "D = A[:, new_order][new_order]\n",
    "\n",
    "ax2.spy(D, precision=0.5, markersize=0.5)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "y = kmeans_spec_assign\n",
    "N = len(y)\n",
    "counts=np.unique(y,return_counts=True)[1][freq]\n",
    "counts_j=np.unique(y, return_counts=True)[1][freq]\n",
    "cum_i=np.cumsum(counts)\n",
    "cum_j=np.cumsum(counts_j)\n",
    "\n",
    "for i in range(cum_i.shape[0]):\n",
    "    x1=np.array([cum_i[i],cum_i[i]])\n",
    "    y1=np.array([[0,N]])\n",
    "    line = plt.Line2D(y1, x1, lw=1.5, color='k', alpha=0.8)\n",
    "    ax2.add_line(line)\n",
    "\n",
    "for i in range(cum_j.shape[0]):\n",
    "    x1=np.array([cum_j[i],cum_j[i]])\n",
    "    y1=np.array([[0,N]])\n",
    "    line = plt.Line2D(x1, y1, lw=1.5, color='k', alpha=0.8)\n",
    "    ax2.add_line(line)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a43aa7",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6bfe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffacd1",
   "metadata": {},
   "source": [
    "The following figures visualize the PCA of the Emails dataset annotated by the ground truth class and the cluster allocations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = np.delete(X.toarray(), removed_idx, axis=0)\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(X_reduced)\n",
    "pca_emb = pca.transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_idx = np.where(labels==0)[0]\n",
    "spam_idx = np.where(labels==1)[0]\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10,10), dpi=100)\n",
    "clust1 = np.where(kmeans_spec_assign==0)[0]\n",
    "clust2 = np.where(kmeans_spec_assign==1)[0]\n",
    "clust3 = np.where(kmeans_spec_assign==2)[0]\n",
    "\n",
    "ax1.scatter(pca_emb[:,0][ham_idx], \n",
    "            pca_emb[:,1][ham_idx],\n",
    "            c='lightblue',\n",
    "            s=5,\n",
    "            label='ham')\n",
    "ax1.scatter(pca_emb[:,0][spam_idx], \n",
    "            pca_emb[:,1][spam_idx],\n",
    "            c='lightcoral',\n",
    "            s=5,\n",
    "            label='spam')\n",
    "\n",
    "ax1.set_xlabel('PC1', fontsize=18)\n",
    "ax1.set_ylabel('PC2', fontsize=18)\n",
    "#ax1.set_title('Ground truth')\n",
    "\n",
    "ax1.legend(prop={'size': 20})\n",
    "fig.savefig('pca_embedding_ground_truth.png', dpi=100)\n",
    "\n",
    "fig, ax2 = plt.subplots(figsize=(10,10), dpi=100)\n",
    "\n",
    "ax2.scatter(pca_emb[:,0][clust2], \n",
    "            pca_emb[:,1][clust2],\n",
    "            c='lightblue',\n",
    "            s=5,\n",
    "            label='cluster 1')\n",
    "ax2.scatter(pca_emb[:,0][clust1], \n",
    "            pca_emb[:,1][clust1],\n",
    "            c='lightcoral',\n",
    "            s=5,\n",
    "            label='cluster 2')\n",
    "ax2.scatter(pca_emb[:,0][clust3], \n",
    "            pca_emb[:,1][clust3],\n",
    "            c='khaki',\n",
    "            s=5,\n",
    "            label='cluster 3')\n",
    "\n",
    "ax2.set_xlabel('PC1', fontsize=18)\n",
    "ax2.set_ylabel('PC2', fontsize=18)\n",
    "#ax2.set_title('Community assignment')\n",
    "legend = ax2.legend(prop={'size': 20})\n",
    "fig.savefig('pca_embedding_community.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6132d",
   "metadata": {},
   "source": [
    "## Missclassifications\n",
    "By inspecting the visualisations of the clusters both in the PCA figures and the reordered pairwise similarity/affinity matrix, it seems as if the spectral clustering algorithmm have identified three clusters. One of which is mainly composed of ham mails, while the other two appear to be composed of spam mails. As a first step let us validate these indications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust1_dist = np.unique(labels[np.where(kmeans_spec_assign==0)[0]], return_counts=True)\n",
    "clust2_dist = np.unique(labels[np.where(kmeans_spec_assign==1)[0]], return_counts=True)\n",
    "clust3_dist = np.unique(labels[np.where(kmeans_spec_assign==2)[0]], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dist in [clust1_dist, clust2_dist, clust3_dist]:\n",
    "    fig, ax = plt.subplots(figsize=(5,5), dpi=100)\n",
    "    bar = ax.bar(['ham', 'spam'], dist[1], color=['lightblue', 'lightcoral'])\n",
    "    \n",
    "    # Add counts above the two bar graphs\n",
    "    for rect in bar:\n",
    "        height = rect.get_height()\n",
    "        plt.text(rect.get_x() + rect.get_width() / 2.0, height, f'{height/(np.sum(dist[1])):.3f}%', ha='center', va='bottom')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab395a93",
   "metadata": {},
   "source": [
    "The above illustrations verify the earlier indications.\n",
    "\n",
    "As the next step in our analysis we will take a deeper look into where the classifications go wrong, and try to deduce why the misclassification might occur. First we will look into the non identified spam mails, that have been assigned to cluster with a majority of ham mails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a1b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "idx = np.where(kmeans_spec_assign==1)[0]\n",
    "clust2 = labels[idx]\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "TDM = pd.DataFrame(X_reduced, columns=feature_names).dropna()\n",
    "miss_classified = TDM.iloc[idx[np.where(clust2==1)[0]], :]\n",
    "\n",
    "clust_cloud = WordCloud(background_color=\"white\", width=1600, height=800).generate_from_frequencies(miss_classified.T.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa138227",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10), dpi=100)\n",
    "ax.imshow(clust_cloud)\n",
    "ax.axis(\"off\")\n",
    "ax.set_title('TFIDF weighted words \\n Missclassified SPAM')\n",
    "#plt.savefig('wordcloud_miss_spam.png',dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8c8aa",
   "metadata": {},
   "source": [
    "It definitely stands out as SPAM. We will try to go through a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3665bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_classified_text = df.drop(removed_idx).iloc[idx[np.where(clust2==1)[0]],:].text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fb8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_classified_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_classified_text[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b9adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_classified_text[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641c697",
   "metadata": {},
   "source": [
    "A possible explanation of the algorithm's shortcoming can be our approach to handle URL links, given that the wordcloud of the TFIDF weighted words highlights the importance of words such as URL or link among the missclasified spam mails. Let us try to investigate the amount of URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5130aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tot_urls = 0\n",
    "for text in miss_classified_text:\n",
    "    tot_urls += len(re.findall(r'(https?://[^\\s]+)', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5078d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The average number of URLs: {tot_urls/len(miss_classified_text):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29513308",
   "metadata": {},
   "source": [
    "Let us compare this number to the spam mails of the two other clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f059ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust1 = labels[np.where(kmeans_spec_assign == 0)[0]]\n",
    "clust3 = labels[np.where(kmeans_spec_assign == 2)[0]]\n",
    "\n",
    "for clust in [clust1, clust3]:\n",
    "    temp_text = df.drop(removed_idx).iloc[idx[np.where(clust==1)[0]],:].text.values\n",
    "\n",
    "    tot_urls = 0\n",
    "    for text in temp_text:\n",
    "        tot_urls += len(re.findall(r'(https?://[^\\s]+)', text))\n",
    "    print(f'The average number of URLs: {tot_urls/len(temp_text):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f5094",
   "metadata": {},
   "source": [
    "Interestingly, one can see that there on average is more links/URLs in the missclassified spam than in the correctly identified spam. Hence, a possible explanation could in fact be that this shortcoming arises due to our initial manipulation of URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f954136",
   "metadata": {},
   "source": [
    "A second thing we can look into is the ham mails being classified as spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9add983",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_classified_c1 = TDM.iloc[idx[np.where(clust1==0)[0]], :]\n",
    "miss_classified_c2 = TDM.iloc[idx[np.where(clust3==0)[0]], :]\n",
    "\n",
    "clust1_cloud = WordCloud(background_color=\"white\", width=1600, height=800).generate_from_frequencies(miss_classified_c1.T.sum(axis=1))\n",
    "clust2_cloud = WordCloud(background_color=\"white\", width=1600, height=800).generate_from_frequencies(miss_classified_c2.T.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3473e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10), dpi=100)\n",
    "ax.imshow(clust1_cloud)\n",
    "ax.axis(\"off\")\n",
    "plt.savefig('wordcloud_miss_hamm_c1.png', dpi=100)\n",
    "#ax.set_title('TFIDF weighted words \\n Missclassified HAM (cluster 1)')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10), dpi=100)\n",
    "ax.imshow(clust2_cloud)\n",
    "ax.axis(\"off\")\n",
    "#ax.set_title('TFIDF weighted words \\n Missclassified HAM (cluster 3)')\n",
    "#fig.tight_layout()\n",
    "plt.savefig('wordcloud_miss_hamm_c2.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1317272",
   "metadata": {},
   "source": [
    "## Wordclouds for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3294d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust1_idx = np.where(kmeans_spec_assign==0)[0]\n",
    "clust2_idx = np.where(kmeans_spec_assign==1)[0]\n",
    "clust3_idx = np.where(kmeans_spec_assign==2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c4f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, clust in enumerate([clust1_idx, clust2_idx, clust3_idx]):\n",
    "    miss_classified = TDM.iloc[clust,:]\n",
    "    clust_cloud = WordCloud(background_color=\"white\", width=1600, height=800).generate_from_frequencies(miss_classified.T.sum(axis=1))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,10), dpi=100)\n",
    "    ax.imshow(clust_cloud)\n",
    "    ax.axis(\"off\")\n",
    "    #ax.set_title(f'TFIDF weighted words: cluster {i}')\n",
    "    plt.savefig(f'wordcloud_cluster_{i+1}.png',dpi=100)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
