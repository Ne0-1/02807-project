{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8803c170",
   "metadata": {},
   "source": [
    "# The supervised models\n",
    "\n",
    "The following models:\n",
    "\n",
    "- Jaccard KNN\n",
    "- TF-IDF KNN\n",
    "- TF-IDF NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c6b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting all parameters\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ddcde3",
   "metadata": {},
   "source": [
    "## Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44e364cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from sklearn.model_selection import RepeatedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import time\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "import mmh3\n",
    "from itertools import product\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import scipy.stats\n",
    "\n",
    "seed = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f24e32",
   "metadata": {},
   "source": [
    "### training function for TF-IDF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09b430fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TDIDF\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "def train(model, model_name, dataset, df_results, seed):\n",
    "    # data\n",
    "    #dataset = 'clean_completeSpamAssassin'\n",
    "    #dataset = 'clean_spam'\n",
    "\n",
    "    if dataset == 'clean_spam':\n",
    "        new_dataset_name = 'SMS'\n",
    "        min_df = 1\n",
    "    else:\n",
    "        new_dataset_name = 'Emails'\n",
    "        min_df = 0.001\n",
    "    # clean_completeSpamAssassin\n",
    "    # clean_spam\n",
    "    data = pd.read_csv(f'../data/{dataset}.csv', encoding='latin')\n",
    "    data.tokens = data.tokens.apply(literal_eval)\n",
    "\n",
    "    df_text_tokenized = {i: [data['text'][i].split(' ')] for i in list(data.index)}\n",
    "    data['text_tokenized'] = pd.DataFrame.from_dict(df_text_tokenized, orient='index')\n",
    "\n",
    "\n",
    "    look_up_for_look_ups = ['raw/BOW', 'raw/Q', 'prep/BOW', 'prep/Q']\n",
    "    look_ups = ['text_tokenized', 'text', 'tokens', 'str_tokens']\n",
    "    dims = {}\n",
    "\n",
    "    #print(f'Model: {model_name}')\n",
    "\n",
    "    for idx, look_up in enumerate(look_ups):\n",
    "        labels = ['ham', 'spam']\n",
    "        q = 5\n",
    "        num_neighs = 5 # 2\n",
    "        num_folds = 5 # 5 fold\n",
    "        dims[look_up] = []\n",
    "        kf = RepeatedKFold(n_splits=num_folds, n_repeats=1, random_state=seed)\n",
    "        f1_scores = []\n",
    "        accuracies = []\n",
    "\n",
    "        for train_idx, test_idx in tqdm(kf.split(data)):\n",
    "            # sorting data\n",
    "            df_train = data.loc[train_idx]\n",
    "            df_test = data.loc[test_idx]\n",
    "            # generating vectors\n",
    "            if look_up in ['text', 'str_tokens']:\n",
    "                # shingles\n",
    "                corpus = list(data[look_up])\n",
    "                vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(q, q), min_df=min_df).fit(corpus)\n",
    "                #vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(q, q), max_df=0.7).fit(corpus)\n",
    "                X = vectorizer.fit_transform(df_train[look_up])\n",
    "                feature_names = vectorizer.get_feature_names_out()\n",
    "                dims[look_up].append(len(feature_names))\n",
    "\n",
    "\n",
    "            else:\n",
    "                vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False, min_df=min_df)\n",
    "                #vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False, max_df=0.7)\n",
    "                vecs = vectorizer.fit_transform(df_train[look_up])\n",
    "                feature_names = vectorizer.get_feature_names_out()\n",
    "                dense = vecs.todense()\n",
    "                lst1 = dense.tolist()\n",
    "                TDM = pd.DataFrame(lst1, columns=feature_names).dropna()\n",
    "                X = vecs\n",
    "                dims[look_up].append(len(feature_names))\n",
    "            # training classifier\n",
    "            model.fit(X, df_train.label)\n",
    "            # predicting\n",
    "            test_vecs = vectorizer.transform(df_test[look_up])\n",
    "            pred_prob = model.predict_proba(test_vecs)\n",
    "            y_pred = [labels[pred_idx] for pred_idx in pred_prob.argmax(axis=1)]\n",
    "            y_test = df_test.label\n",
    "            # generating predictions\n",
    "            f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1_scores.append(f1_score(y_test, y_pred, pos_label='spam'))\n",
    "            accuracies.append(accuracy_score(y_test, y_pred))\n",
    "        #print(look_up)\n",
    "        #print(f'Dataset: {look_up_for_look_ups[idx]}')\n",
    "        #print(f'f1_mean: {np.mean(f1_scores)}  |  f1: {f1_scores}')\n",
    "        #print(f'acc_mean: {np.mean(accuracies)}  |  acc: {accuracies}')\n",
    "        #print('\\n')\n",
    "        \n",
    "        single_df_results = pd.DataFrame({'Model': [model_name]*num_results,\n",
    "                                      'Dataset': [new_dataset_name]*num_results, \n",
    "                                      'Method': [look_up_for_look_ups[idx]]*num_results,\n",
    "                                      'F1': f1_scores,\n",
    "                                      'Accuracy': accuracies,\n",
    "                                      })\n",
    "        df_results = df_results.append(single_df_results)\n",
    "\n",
    "        \n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e2461",
   "metadata": {},
   "source": [
    "### dataframe with results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20d8a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general dataframe with results\n",
    "df_template = pd.DataFrame(columns=[\"Model\", \"Dataset\", \"Method\", 'F1', 'Accuracy'])    # from low to high conf\n",
    "df_results = df_template\n",
    "num_results = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa0b45",
   "metadata": {},
   "source": [
    "## Evaluating the KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57c7f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defnining model\n",
    "num_neighs = 5\n",
    "model = KNeighborsClassifier(n_neighbors=num_neighs, metric='cosine')\n",
    "model_name = 'TF-IDF KNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32f20587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:20, 16.16s/it]\n",
      "5it [00:03,  1.38it/s]\n",
      "5it [00:23,  4.62s/it]\n",
      "5it [00:01,  2.56it/s]\n",
      "5it [01:55, 23.02s/it]\n",
      "5it [01:00, 12.04s/it]\n",
      "5it [00:40,  8.12s/it]\n",
      "5it [00:24,  4.95s/it]\n"
     ]
    }
   ],
   "source": [
    "# running model on each dataset and for different data processing methods\n",
    "datasets = ['clean_spam', 'clean_completeSpamAssassin']\n",
    "for dataset in datasets:\n",
    "    model_df_results = train(model, model_name, dataset, df_template, seed)\n",
    "    df_results = df_results.append(model_df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d025c5a",
   "metadata": {},
   "source": [
    "#### Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba05c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the results\n",
    "df_results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4747070f",
   "metadata": {},
   "source": [
    "## Evaluating the NB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd73c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.read_csv(f'results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b47c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defnining model\n",
    "# note the prior is 50/50, achieve better performance, i.e. f1 score\n",
    "model = MultinomialNB(fit_prior=False)\n",
    "model_name = 'TF-IDF NB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3192da3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:26, 17.22s/it]\n",
      "5it [00:03,  1.50it/s]\n",
      "5it [00:27,  5.41s/it]\n",
      "5it [00:01,  2.85it/s]\n",
      "5it [01:53, 22.62s/it]\n",
      "5it [00:55, 11.04s/it]\n",
      "5it [00:35,  7.09s/it]\n",
      "5it [00:23,  4.68s/it]\n"
     ]
    }
   ],
   "source": [
    "# running model on each dataset and for different data processing methods\n",
    "datasets = ['clean_spam', 'clean_completeSpamAssassin']\n",
    "for dataset in datasets:\n",
    "    model_df_results = train(model, model_name, dataset, df_template, seed)\n",
    "    df_results = df_results.append(model_df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83022939",
   "metadata": {},
   "source": [
    "#### Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c384926",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the results\n",
    "df_results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321a459",
   "metadata": {},
   "source": [
    "## Evaluating the Jaccard KNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f991438",
   "metadata": {},
   "source": [
    "We start by defining the necessary functions to get document similarity (week 5 of course exercises). Please note that the listhash, minhash, and signatures are gone. We will not be using them in this model as we saw a decrease in model performance. We did not perform any extensive analysis and we believe this is due to the information loss as we minhash. And as we won't minhash there is no need to create hashes at all and therefore, there won't be any signatures. Just q-shingles!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b172eac9",
   "metadata": {},
   "source": [
    "### Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "34cbac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create shingle function\n",
    "def shingles(string:str,q:int):\n",
    "    output = set()\n",
    "    for i in range(len(string)+1):\n",
    "        if i < q:\n",
    "            pass\n",
    "        else:\n",
    "            output.add(''.join(string[i-q:i]))\n",
    "    return output\n",
    "\n",
    "#create jaccard sim function\n",
    "def jaccard(doc1, doc2):\n",
    "    intersect = np.intersect1d(doc1,doc2)\n",
    "    union = np.union1d(doc1,doc2)\n",
    "    if len(union) != 0:\n",
    "        return len(intersect) / len(union)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#create document sim function\n",
    "def similarity(docs:dict):\n",
    "    output = np.zeros((len(docs.keys()),len(docs.keys())))\n",
    "    for key1, value1 in tqdm(docs.items()):\n",
    "        for key2, value2 in docs.items():\n",
    "            if key1 <= key2:\n",
    "                pass\n",
    "            else:\n",
    "                jac_value = jaccard(np.array(value1),np.array(value2))\n",
    "                output[key1,key2] = jac_value\n",
    "    return np.tril(output) + np.triu(output.T, 1)\n",
    "\n",
    "\n",
    "# the model\n",
    "def weighted_knn(x, y_train,test_idx,low,high,k_neighbours=5):\n",
    "    y_test = []\n",
    "    mask = np.ones(len(x),bool)\n",
    "    mask[y_test] = False\n",
    "    for i in test_idx:\n",
    "        ind = []\n",
    "        temp = np.argpartition(x[i], -k_neighbours)[-k_neighbours:]\n",
    "        temp = np.flip(temp)\n",
    "        for idx in temp:\n",
    "            if idx>=high or idx < low:\n",
    "                ind.append(idx)\n",
    "        topk = x[i][ind]\n",
    "        labels = {j: y_train[j] for j in ind}\n",
    "        ham = 0\n",
    "        spam = 0\n",
    "        for key, value in labels.items():\n",
    "            if value == 0:\n",
    "                ham += x[i][key]\n",
    "            if value == 1:\n",
    "                spam += x[i][key]\n",
    "        \n",
    "        if ham>spam:\n",
    "            y_test.append(0)\n",
    "        if ham == spam:\n",
    "            y_test.append(0)\n",
    "        if ham<spam:\n",
    "            y_test.append(1)\n",
    "        \n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e3126",
   "metadata": {},
   "source": [
    "Now we create the \"training\" loop. The training actually happens as we create the document similarity matrix. This function just evaluates one fold of a cross validation ind gives us the predicted labels for the test set in that fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49132c72",
   "metadata": {},
   "source": [
    "Now we get to a piece of code that we are not really proud of. However, we decided to use no sklearn models in this section (except for evaluating f1-scores and accuracies). We evaluate the model with a $k$ (in KNN) of 5 and a $q$ of 5 as that was suggested by the course. We evaluate through a 5-fold cross validation and save all performance metrics from each run.\n",
    "\n",
    "We also keep the predictions of the outer test set in order to do McNemar tests between models. This will be done in another notebook as this one has become quite extensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "901d9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jknn_train(model_name, dataset, df_results, seed):\n",
    "    # data\n",
    "    #dataset = 'clean_completeSpamAssassin'\n",
    "    #dataset = 'clean_spam'\n",
    "\n",
    "    if dataset == 'clean_spam':\n",
    "        new_dataset_name = 'SMS'\n",
    "        min_df = 1\n",
    "    else:\n",
    "        new_dataset_name = 'Emails'\n",
    "        min_df = 0.001\n",
    "    # clean_completeSpamAssassin\n",
    "    # clean_spam\n",
    "    data = pd.read_csv(f'../data/{dataset}.csv', encoding='latin')\n",
    "    data.tokens = data.tokens.apply(literal_eval)\n",
    "    \n",
    "    df_text_tokenized = {i: [data['text'][i].split(' ')] for i in list(data.index)}\n",
    "    data['text_tokenized'] = pd.DataFrame.from_dict(df_text_tokenized, orient='index')\n",
    "\n",
    "\n",
    "    look_up_for_look_ups = ['raw/BOW', 'raw/Q', 'prep/BOW', 'prep/Q']\n",
    "    look_ups = ['text_tokenized', 'text', 'tokens', 'str_tokens']\n",
    "    \n",
    "    q=5\n",
    "    \n",
    "    for idx, look_up in enumerate(look_ups):\n",
    "        # data is in strings\n",
    "        if look_ups in ['text', 'str_tokens']:\n",
    "            curr_data = {data.index[_]: shingles(data.iloc[:,1][_], q=q) for _ in data.index}\n",
    "        # data is in tokens\n",
    "        else:\n",
    "            curr_data = data[look_up]\n",
    "             \n",
    "        sim_matrix = similarity(curr_data)\n",
    "        kfold=5\n",
    "        f1_scores = []\n",
    "        accuracies = []\n",
    "        test_set_percent = ((len(data)/kfold)/len(data))\n",
    "        test_size = int(test_set_percent*len(data))\n",
    "        former_test_idx = 0\n",
    "        y_tests = []\n",
    "        predicted = []\n",
    "        for i in range(kfold):\n",
    "            y_test = data['binary'][former_test_idx:(i+1)*test_size]\n",
    "            y_tests = y_tests + list(y_test)\n",
    "            y_idx = y_test.index\n",
    "            mask = np.ones(len(data), bool)\n",
    "            mask[y_idx] = False\n",
    "            y_train = data['binary'][mask]\n",
    "            y_pred = weighted_knn(sim_matrix,y_train,y_idx,former_test_idx,(i+1)*test_size,5)\n",
    "            predicted = predicted + list(y_pred)\n",
    "            f1_scores.append(f1_score(y_test,y_pred))\n",
    "            accuracies.append(accuracy_score(y_test,y_pred))\n",
    "            former_test_idx += test_size\n",
    "\n",
    "        # saving look up, i.e. method\n",
    "        single_df_results = pd.DataFrame({'Model': [model_name]*num_results,\n",
    "                                  'Dataset': [new_dataset_name]*num_results, \n",
    "                                  'Method': [look_up_for_look_ups[idx]]*num_results,\n",
    "                                  'F1': f1_scores,\n",
    "                                  'Accuracy': accuracies,\n",
    "                                  })\n",
    "        df_results = df_results.append(single_df_results)\n",
    "\n",
    "    return df_results\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f52cbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading results\n",
    "df_results = pd.read_csv(f'results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d25cf1",
   "metadata": {},
   "source": [
    "## Evaluating the J KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "49da67ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 626.34it/s]\n",
      "100it [00:00, 986.86it/s]\n",
      "100it [00:00, 796.98it/s]\n",
      "100it [00:00, 1008.90it/s]\n",
      "100it [00:01, 54.02it/s]\n",
      "100it [00:00, 684.10it/s]\n",
      "100it [00:00, 202.27it/s]\n",
      "100it [00:00, 823.71it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'J KNN'\n",
    "# running model on each dataset and for different data processing methods\n",
    "datasets = ['clean_spam', 'clean_completeSpamAssassin']\n",
    "for dataset in datasets:\n",
    "    model_df_results = jknn_train(model_name, dataset, df_template, seed)\n",
    "    df_results = df_results.append(model_df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575f2c2",
   "metadata": {},
   "source": [
    "### Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the results\n",
    "df_results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612fa51",
   "metadata": {},
   "source": [
    "## Analysing the results\n",
    "\n",
    "With confidence level of 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "006958b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "round = 3 # round numbers\n",
    "data = pd.read_csv(f'results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a067c693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF NB\n",
      "  Emails\n",
      "    raw/Q: f1: 0.895, [0.874,0.917] | acc: 0.946, [0.939,0.954]\n",
      "    prep/Q: f1: 0.959, [0.95,0.968] | acc: 0.977, [0.972,0.983]\n",
      "    raw/BOW: f1: 0.944, [0.931,0.956] | acc: 0.97, [0.963,0.976]\n",
      "    prep/BOW: f1: 0.953, [0.937,0.97] | acc: 0.973, [0.964,0.982]\n",
      "  SMS\n",
      "    raw/Q: f1: 0.908, [0.871,0.944] | acc: 0.977, [0.968,0.986]\n",
      "    prep/Q: f1: 0.842, [0.801,0.882] | acc: 0.955, [0.945,0.965]\n",
      "    raw/BOW: f1: 0.882, [0.848,0.917] | acc: 0.97, [0.96,0.98]\n",
      "    prep/BOW: f1: 0.841, [0.814,0.867] | acc: 0.953, [0.948,0.958]\n",
      "\n",
      "\n",
      "TF-IDF KNN\n",
      "  Emails\n",
      "    raw/Q: f1: 0.86, [0.812,0.907] | acc: 0.914, [0.887,0.941]\n",
      "    prep/Q: f1: 0.938, [0.919,0.957] | acc: 0.965, [0.953,0.976]\n",
      "    raw/BOW: f1: 0.897, [0.883,0.911] | acc: 0.94, [0.935,0.945]\n",
      "    prep/BOW: f1: 0.933, [0.923,0.943] | acc: 0.962, [0.955,0.968]\n",
      "  SMS\n",
      "    raw/Q: f1: 0.901, [0.871,0.932] | acc: 0.975, [0.968,0.983]\n",
      "    prep/Q: f1: 0.85, [0.804,0.897] | acc: 0.963, [0.95,0.975]\n",
      "    raw/BOW: f1: 0.852, [0.813,0.89] | acc: 0.965, [0.955,0.974]\n",
      "    prep/BOW: f1: 0.855, [0.834,0.876] | acc: 0.965, [0.96,0.971]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def mean_confidence_interval(data, round=3, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return np.round(m, round), np.round(m-h, round), np.round(m+h, round)\n",
    "\n",
    "\n",
    "models = list(set(data.Model))\n",
    "datasets = list(set(data.Dataset))\n",
    "# force order\n",
    "methods = ['raw/Q', 'prep/Q', 'raw/BOW', 'prep/BOW']\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    for dataset in datasets:\n",
    "        print(f'  {dataset}')\n",
    "        for method in methods:\n",
    "            current_data = data[(data.Model==model) & (data.Dataset==dataset) & (data.Method==method)]\n",
    "            f1, f1_l, f1_h = mean_confidence_interval(list(current_data.F1), round=round)\n",
    "            acc, acc_l, acc_h = mean_confidence_interval(list(current_data.Accuracy), round=round)\n",
    "            # todo: round\n",
    "            print(f'    {method}: f1: {f1}, [{f1_l},{f1_h}] | acc: {acc}, [{acc_l},{acc_h}]')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c235a5",
   "metadata": {},
   "source": [
    "## resetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac879a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting all parameters\n",
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
