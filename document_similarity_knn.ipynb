{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b11a72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import mmh3\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0e3ecc",
   "metadata": {},
   "source": [
    "We start by defining the necessary functions to get document similarity (week 5 of course exercises)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3ccde178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create shingle function\n",
    "def shingles(string:str,q:int):\n",
    "    output = set()\n",
    "    for i in range(len(string)+1):\n",
    "        if i < q:\n",
    "            pass\n",
    "        else:\n",
    "            output.add(' '.join(string[i-q:i]))\n",
    "    return output\n",
    "\n",
    "#create listhash function\n",
    "def listhash(l,seeds):\n",
    "    vals = set()\n",
    "\n",
    "    for e in l:\n",
    "        val = 0\n",
    "        for seed in seeds:\n",
    "            val = val ^ mmh3.hash(e, seed)\n",
    "        vals.add(val)\n",
    "    return vals\n",
    "\n",
    "#create signatures function\n",
    "def signatures(docs, q=9, k=20):\n",
    "    sign = {}\n",
    "    for key, value in docs.items():\n",
    "        sign[key] = listhash(shingles(value,q=q),np.arange(k))\n",
    "    return sign\n",
    "\n",
    "#create jaccard sim function\n",
    "def jaccard(doc1, doc2):\n",
    "    doc1=set(doc1)\n",
    "    doc2=set(doc2)\n",
    "    intersect = doc1.intersection(doc2)\n",
    "    union = doc1.union(doc2)\n",
    "    if len(union) != 0:\n",
    "        return len(intersect) / len(union)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#create document sim function\n",
    "def similarity(docs:dict):\n",
    "    output = np.zeros((len(docs.keys()),len(docs.keys())))\n",
    "    for key1, value1 in docs.items():\n",
    "        for key2, value2 in docs.items():\n",
    "            if key1 == key2:\n",
    "                pass\n",
    "            else:\n",
    "                jac_value = jaccard(value1,value2)\n",
    "                output[key1,key2] = jac_value\n",
    "    return output\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a3502b",
   "metadata": {},
   "source": [
    "Load email data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "65b66cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "emails = pd.read_csv('data/clean_spam.csv', encoding='latin')\n",
    "emails.tokens = emails.tokens.apply(literal_eval) #convert tokens to list of tokens\n",
    "\n",
    "#determine q and k\n",
    "q = 5 #number of characters in each shingle\n",
    "k = 20 #number of hashes per shingle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "9e5ef3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create signatures for emails (we keep count based on index in emails)\n",
    "email_signatures = {emails.index[i]: listhash(shingles(emails['text'][i], q=q),np.arange(k)) for i in emails.index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1fd76ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create signatures for emails\n",
    "email_similarity = similarity(email_signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "8456c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_knn(x, y_train,test_idx,low,high,k_neighbours=5):\n",
    "    y_test = []\n",
    "    mask = np.ones(len(x),bool)\n",
    "    mask[y_test] = False\n",
    "    for i in test_idx:\n",
    "        ind = []\n",
    "        temp = np.argpartition(x[i], -k_neighbours)[-k_neighbours:]\n",
    "        temp = np.flip(temp)\n",
    "        for idx in temp:\n",
    "            if idx>=high or idx < low:\n",
    "                ind.append(idx)\n",
    "        topk = x[i][ind]\n",
    "        labels = {j: y_train[j] for j in ind}\n",
    "        ham = 0\n",
    "        spam = 0\n",
    "        for key, value in labels.items():\n",
    "            if value == 'ham':\n",
    "                ham += x[i][key]\n",
    "            if value == 'spam':\n",
    "                spam += x[i][key]\n",
    "        if ham>spam:\n",
    "            y_test.append('ham')\n",
    "        if ham == spam:\n",
    "            y_test.append('ham')\n",
    "        if ham<spam:\n",
    "            y_test.append('spam')\n",
    "    return y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "f41e2730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1-score: 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "kfold=5\n",
    "f1_scores = []\n",
    "test_set_percent = ((len(emails)/kfold)/len(emails))\n",
    "test_size = round(test_set_percent*len(emails))\n",
    "former_test_idx = 0\n",
    "y_pred1 = []\n",
    "for i in range(kfold):\n",
    "    y_test = emails['label'][former_test_idx:(i+1)*test_size]\n",
    "    y_idx = y_test.index\n",
    "    mask = np.ones(len(emails), bool)\n",
    "    mask[y_idx] = False\n",
    "    y_train = emails['label'][mask]\n",
    "    y_pred = weighted_knn(email_similarity,y_train,y_idx,former_test_idx,(i+1)*test_size,5)\n",
    "    y_pred1 = y_pred1 + list(y_pred)\n",
    "    f1_scores.append(f1_score(y_test,y_pred, pos_label='spam'))\n",
    "    former_test_idx += test_size\n",
    "print(f'Average f1-score: {round(np.mean(f1_scores),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445a6da",
   "metadata": {},
   "source": [
    "Lets try the same with bag of words instead of signatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "a760e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get bag of words as sets for each email\n",
    "email_bow = {emails.index[i]: set(emails['tokens'][i]) for i in range(len(emails))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "c51d2aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get similarity between documents\n",
    "email_bow_sim = similarity(email_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "f3cc4c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1-score: 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "kfold=5\n",
    "f1_scores = []\n",
    "test_set_percent = ((len(emails)/kfold)/len(emails))\n",
    "test_size = round(test_set_percent*len(emails))\n",
    "former_test_idx = 0\n",
    "y_tests = []\n",
    "y_pred2 = []\n",
    "for i in range(kfold):\n",
    "    y_test = emails['label'][former_test_idx:(i+1)*test_size]\n",
    "    y_tests = y_tests + list(y_test)\n",
    "    y_idx = y_test.index\n",
    "    mask = np.ones(len(emails), bool)\n",
    "    mask[y_idx] = False\n",
    "    y_train = emails['label'][mask]\n",
    "    y_pred = weighted_knn(email_bow_sim,y_train,y_idx,former_test_idx,(i+1)*test_size,5)\n",
    "    y_pred2 = y_pred2 + list(y_pred)\n",
    "    f1_scores.append(f1_score(y_test,y_pred, pos_label='spam'))\n",
    "    former_test_idx += test_size\n",
    "print(f'Average f1-score: {round(np.mean(f1_scores),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc6d0e",
   "metadata": {},
   "source": [
    "So, the model with signatures performs better, but is this significant? We will perform a McNemar test to evaluate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "f3b58f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "#define McNemar test (inspired by a function made in Introduction to Machine Learning)\n",
    "def mcnemar(y_true, y_pred1, y_pred2, alpha=0.05):\n",
    "    nn = np.zeros((2,2))\n",
    "    c1 = y_pred1 - y_true == 0\n",
    "    c2 = y_pred2 - y_true == 0\n",
    "\n",
    "    nn[0,0] = sum(c1 & c2)\n",
    "    nn[0,1] = sum(c1 & ~c2)\n",
    "    nn[1,0] = sum(~c1 & c2)\n",
    "    nn[1,1] = sum(~c1 & ~c2)\n",
    "\n",
    "    n = sum(nn.flat);\n",
    "    n12 = nn[0,1]\n",
    "    n21 = nn[1,0]\n",
    "\n",
    "    thetahat = (n12-n21)/n\n",
    "    Etheta = thetahat\n",
    "\n",
    "    Q = n**2 * (n+1) * (Etheta+1) * (1-Etheta) / ( (n*(n12+n21) - (n12-n21)**2) )\n",
    "\n",
    "    p = (Etheta + 1)*0.5 * (Q-1)\n",
    "    q = (1-Etheta)*0.5 * (Q-1)\n",
    "\n",
    "    CI = tuple(lm * 2 - 1 for lm in scipy.stats.beta.interval(1-alpha, a=p, b=q) )\n",
    "\n",
    "    p = 2*scipy.stats.binom.cdf(min([n12,n21]), n=n12+n21, p=0.5)\n",
    "    print(r'Result of McNemars test using $\\alpha$ = ', alpha)\n",
    "    print('Comparison matrix n')\n",
    "    print(nn)\n",
    "    if n12+n21 <= 10:\n",
    "        print('Warning, n12+n21 is low: n12+n21=',(n12+n21))\n",
    "    print(r'$\\theta_hat$: ',thetahat)\n",
    "    print(r'Approximate 1-$\\alpha$ confidence interval of $\\theta$: [$\\theta_L$,$\\theta_U$] = ', CI)\n",
    "    print(r'p-value for two-sided test model 1 and model 2 have same accuracy (exact binomial test): p = ', p)\n",
    "\n",
    "    return thetahat, CI, p "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f4ed15",
   "metadata": {},
   "source": [
    "Because of the design of this function, we need to vstack all y_test and y_pred to accomodate for all folds in the cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "6c33fdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of McNemars test using $\\alpha$ =  0.05\n",
      "Comparison matrix n\n",
      "[[5408.   75.]\n",
      " [  37.   50.]]\n",
      "$\\theta_hat$:  0.006822262118491921\n",
      "Approximate 1-$\\alpha$ confidence interval of $\\theta$: [$\\theta_L$,$\\theta_U$] =  (0.003102931217386473, 0.010541499940176058)\n",
      "p-value for two-sided test model 1 and model 2 have same accuracy (exact binomial test): p =  0.0004209679285168773\n"
     ]
    }
   ],
   "source": [
    "np_y_tests = np.zeros(len(y_tests))\n",
    "np_y_pred1 = np.zeros(len(y_tests))\n",
    "np_y_pred2 = np.zeros(len(y_tests))\n",
    "\n",
    "for i in range(len(y_tests)):\n",
    "    if y_tests[i] == 'spam':\n",
    "        np_y_tests[i] = 1\n",
    "    if y_pred1[i] == 'spam':\n",
    "        np_y_pred1[i] = 1\n",
    "    if y_pred2[i] == 'spam':\n",
    "        np_y_pred2[i] = 1\n",
    "\n",
    "\n",
    "mcresults = mcnemar(np_y_tests, np_y_pred1, np_y_pred2, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b8091d",
   "metadata": {},
   "source": [
    "You can see that the p-value is very much lower than our level of confidence (alpha) of 0.05. Therefore, we can reject the null-hypothesis that the accuracies of the two models are equal!\n",
    "\n",
    "Translated to English, this means that the first model based on signatures is significantly better than the model based on tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20036138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
